if(void 0===__webpack_require__)var __webpack_require__={};(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[2413],{8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>l});var s=i(6540);const t={},o=s.createContext(t);function r(e){const n=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),s.createElement(o.Provider,{value:n},e.children)}},8457:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>h,frontMatter:()=>r,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"chapter-07/index","title":"Chapter 7: Vision-Language-Action (VLA)","description":"Integrate LLMs with robotics: voice commands, cognitive planning, and multi-modal interaction","source":"@site/i18n/ur/docusaurus-plugin-content-docs/current/chapter-07/index.md","sourceDirName":"chapter-07","slug":"/chapter-07/","permalink":"/physical-ai-book/ur/docs/chapter-07/","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":7,"frontMatter":{"title":"Chapter 7: Vision-Language-Action (VLA)","description":"Integrate LLMs with robotics: voice commands, cognitive planning, and multi-modal interaction","sidebar_position":7},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 6.5: Human-Robot Interaction","permalink":"/physical-ai-book/ur/docs/chapter-06/lesson-05"},"next":{"title":"Chapter 7 Quiz","permalink":"/physical-ai-book/ur/docs/chapter-07/quiz"}}');var t=i(4848),o=i(8453);const r={title:"Chapter 7: Vision-Language-Action (VLA)",description:"Integrate LLMs with robotics: voice commands, cognitive planning, and multi-modal interaction",sidebar_position:7},l="Chapter 7: Vision-Language-Action (VLA)",a={},c=[{value:"Overview",id:"overview",level:2},{value:"Chapter Learning Outcomes",id:"chapter-learning-outcomes",level:2},{value:"Lessons",id:"lessons",level:2},{value:"Lesson 7.1: Voice-to-Action with OpenAI Whisper",id:"lesson-71-voice-to-action-with-openai-whisper",level:3},{value:"Lesson 7.2: LLM-Based Cognitive Planning",id:"lesson-72-llm-based-cognitive-planning",level:3},{value:"Lesson 7.3: Natural Language to ROS 2 Actions",id:"lesson-73-natural-language-to-ros-2-actions",level:3},{value:"Lesson 7.4: Multi-Modal Interaction",id:"lesson-74-multi-modal-interaction",level:3},{value:"Lesson 7.5: Capstone Project - The Autonomous Humanoid",id:"lesson-75-capstone-project---the-autonomous-humanoid",level:3},{value:"Chapter Assessment",id:"chapter-assessment",level:2},{value:"Chapter 7 Quiz",id:"chapter-7-quiz",level:3},{value:"Capstone Project Evaluation",id:"capstone-project-evaluation",level:3},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Proficiency Mapping",id:"proficiency-mapping",level:2},{value:"\ud83c\udfaf Capstone Project Overview",id:"-capstone-project-overview",level:2},{value:"Project Goal",id:"project-goal",level:3},{value:"Project Components",id:"project-components",level:3},{value:"Deliverables",id:"deliverables",level:3},{value:"\ud83d\udcdd Content Creation Workflow",id:"-content-creation-workflow",level:2}];function d(e){const n={a:"a",br:"br",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"chapter-7-vision-language-action-vla",children:"Chapter 7: Vision-Language-Action (VLA)"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Part of"}),": Module 4 - Vision-Language-Action (VLA) | Week 13",(0,t.jsx)(n.br,{}),"\n",(0,t.jsx)(n.strong,{children:"Course Structure"}),": This chapter covers the convergence of LLMs and Robotics, enabling natural language control of humanoid robots."]}),"\n",(0,t.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Vision-Language-Action (VLA)"})," represents the cutting edge of Physical AI\u2014combining large language models (LLMs) with robot perception and control. This module enables robots to understand natural language commands, reason about tasks, and execute complex behaviors autonomously."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"What you'll master:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Voice-to-Action: Using OpenAI Whisper for voice commands"}),"\n",(0,t.jsx)(n.li,{children:"Cognitive Planning: Using LLMs to translate natural language into ROS 2 actions"}),"\n",(0,t.jsx)(n.li,{children:"Multi-modal interaction: speech, gesture, vision"}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Capstone Project"}),": The Autonomous Humanoid - A complete system where a simulated robot receives a voice command, plans a path, navigates obstacles, identifies an object using computer vision, and manipulates it"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"By the end, you'll build a complete conversational humanoid robot system!"}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"chapter-learning-outcomes",children:"Chapter Learning Outcomes"}),"\n",(0,t.jsx)(n.p,{children:"After completing this chapter, you will be able to:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Integrate Whisper"})," for speech recognition and voice command processing (CEFR: B2, Bloom's: Apply)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Use LLMs"})," for cognitive planning and task decomposition (CEFR: B2, Bloom's: Apply)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Translate natural language"})," into ROS 2 action sequences (CEFR: B2, Bloom's: Apply)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Implement multi-modal interaction"})," combining speech, vision, and gesture (CEFR: B2, Bloom's: Apply)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Build complete VLA systems"})," from voice input to robot execution (CEFR: B2+, Bloom's: Create)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Complete Capstone Project"}),": Autonomous humanoid with full VLA pipeline (CEFR: B2+, Bloom's: Create)"]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"lessons",children:"Lessons"}),"\n",(0,t.jsx)(n.h3,{id:"lesson-71-voice-to-action-with-openai-whisper",children:(0,t.jsx)(n.a,{href:"/physical-ai-book/ur/docs/chapter-07/lesson-01-whisper-voice",children:"Lesson 7.1: Voice-to-Action with OpenAI Whisper"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Duration"}),": 70 minutes | ",(0,t.jsx)(n.strong,{children:"Difficulty"}),": B2 (Advanced)"]}),"\n",(0,t.jsx)(n.p,{children:"Learn to use OpenAI Whisper for speech recognition. Convert voice commands into text that robots can understand."}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Key Concepts"}),": Whisper API, speech-to-text, voice command processing, ROS 2 audio topics, real-time transcription"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Exercises"}),": 4 interactive Python exercises + 2 Try With AI"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h3,{id:"lesson-72-llm-based-cognitive-planning",children:(0,t.jsx)(n.a,{href:"/physical-ai-book/ur/docs/chapter-07/lesson-02-llm-planning",children:"Lesson 7.2: LLM-Based Cognitive Planning"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Duration"}),": 70 minutes | ",(0,t.jsx)(n.strong,{children:"Difficulty"}),": B2 (Advanced)"]}),"\n",(0,t.jsx)(n.p,{children:"Use GPT models to translate natural language commands into structured robot action plans. Implement cognitive planning for complex tasks."}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Key Concepts"}),": LLM integration, prompt engineering, task decomposition, action sequence generation, ROS 2 action planning"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Exercises"}),": 4 interactive Python exercises + 2 Try With AI"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h3,{id:"lesson-73-natural-language-to-ros-2-actions",children:(0,t.jsx)(n.a,{href:"/physical-ai-book/ur/docs/chapter-07/lesson-03-nl-to-ros2",children:"Lesson 7.3: Natural Language to ROS 2 Actions"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Duration"}),": 60 minutes | ",(0,t.jsx)(n.strong,{children:"Difficulty"}),": B2 (Advanced)"]}),"\n",(0,t.jsx)(n.p,{children:"Bridge the gap between natural language and ROS 2. Convert LLM-generated plans into executable ROS 2 actions and services."}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Key Concepts"}),": ROS 2 action clients, service calls, command translation, error handling, safety validation"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Exercises"}),": 3 interactive Python exercises + 1 Try With AI"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h3,{id:"lesson-74-multi-modal-interaction",children:(0,t.jsx)(n.a,{href:"/physical-ai-book/ur/docs/chapter-07/lesson-04-multimodal",children:"Lesson 7.4: Multi-Modal Interaction"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Duration"}),": 60 minutes | ",(0,t.jsx)(n.strong,{children:"Difficulty"}),": B2+ (Upper Advanced)"]}),"\n",(0,t.jsx)(n.p,{children:"Combine speech, vision, and gesture for natural human-robot interaction. Implement multi-modal perception and response."}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Key Concepts"}),": Sensor fusion, multi-modal perception, gesture recognition, visual attention, conversational interfaces"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Exercises"}),": 3 interactive Python exercises + 2 Try With AI"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h3,{id:"lesson-75-capstone-project---the-autonomous-humanoid",children:(0,t.jsx)(n.a,{href:"/physical-ai-book/ur/docs/chapter-07/lesson-05-capstone",children:"Lesson 7.5: Capstone Project - The Autonomous Humanoid"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Duration"}),": 120 minutes | ",(0,t.jsx)(n.strong,{children:"Difficulty"}),": B2+ (Upper Advanced)"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"The Capstone Project"}),": Build a complete autonomous humanoid system that:"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:'Receives a voice command (e.g., "Clean the room")'}),"\n",(0,t.jsx)(n.li,{children:"Plans a path using Nav2"}),"\n",(0,t.jsx)(n.li,{children:"Navigates obstacles"}),"\n",(0,t.jsx)(n.li,{children:"Identifies target objects using computer vision"}),"\n",(0,t.jsx)(n.li,{children:"Manipulates objects with humanoid hands"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Key Concepts"}),": System integration, end-to-end pipeline, testing and validation, deployment"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Exercises"}),": Complete project implementation + 3 Try With AI"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"chapter-assessment",children:"Chapter Assessment"}),"\n",(0,t.jsx)(n.h3,{id:"chapter-7-quiz",children:(0,t.jsx)(n.a,{href:"/physical-ai-book/ur/docs/chapter-07/quiz",children:"Chapter 7 Quiz"})}),"\n",(0,t.jsx)(n.p,{children:"Test your understanding with a comprehensive quiz covering VLA concepts and implementation."}),"\n",(0,t.jsx)(n.h3,{id:"capstone-project-evaluation",children:"Capstone Project Evaluation"}),"\n",(0,t.jsx)(n.p,{children:"The Capstone Project will be evaluated on:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Functionality"}),": System works end-to-end (40%)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Code Quality"}),": Clean, documented, modular code (20%)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Innovation"}),": Creative solutions and extensions (20%)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Presentation"}),": Clear demonstration and explanation (20%)"]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsx)(n.p,{children:"This chapter builds on all previous chapters. You should be comfortable with:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Chapter 1"}),": Physical AI fundamentals"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Chapter 2"}),": ROS 2 nodes, topics, services, actions"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Chapter 3"}),": Gazebo simulation"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Chapter 4"}),": Reinforcement learning and Isaac Sim"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Chapter 5"}),": Motion planning and Nav2"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Chapter 6"}),": Humanoid robot control"]}),"\n",(0,t.jsx)(n.li,{children:"Python programming (intermediate level)"}),"\n",(0,t.jsx)(n.li,{children:"Basic understanding of LLMs and APIs"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Note"}),": This chapter requires API access to OpenAI (Whisper and GPT models). Free tier may have rate limits."]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"proficiency-mapping",children:"Proficiency Mapping"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Aspect"}),(0,t.jsx)(n.th,{children:"Level"}),(0,t.jsx)(n.th,{children:"Framework"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Language Complexity"}),(0,t.jsx)(n.td,{children:"B2-B2+"}),(0,t.jsx)(n.td,{children:"CEFR"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Cognitive Demand"}),(0,t.jsx)(n.td,{children:"Apply \u2192 Create"}),(0,t.jsx)(n.td,{children:"Bloom's Taxonomy"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Digital Skills"}),(0,t.jsx)(n.td,{children:"5-6 (Advanced)"}),(0,t.jsx)(n.td,{children:"DigComp 2.2"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Prerequisites"}),(0,t.jsx)(n.td,{children:"All previous chapters"}),(0,t.jsx)(n.td,{children:"-"})]})]})]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"-capstone-project-overview",children:"\ud83c\udfaf Capstone Project Overview"}),"\n",(0,t.jsx)(n.h3,{id:"project-goal",children:"Project Goal"}),"\n",(0,t.jsx)(n.p,{children:"Build an autonomous humanoid robot that can:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Understand natural language voice commands"}),"\n",(0,t.jsx)(n.li,{children:"Plan and execute complex tasks"}),"\n",(0,t.jsx)(n.li,{children:"Navigate environments safely"}),"\n",(0,t.jsx)(n.li,{children:"Identify and manipulate objects"}),"\n",(0,t.jsx)(n.li,{children:"Interact naturally with humans"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"project-components",children:"Project Components"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Voice Interface"}),": Whisper-based speech recognition"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Cognitive Planner"}),": LLM-based task decomposition"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Navigation System"}),": Nav2 path planning for humanoids"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Perception Pipeline"}),": Computer vision for object detection"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Manipulation System"}),": Humanoid hand control for grasping"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Integration Layer"}),": ROS 2 orchestration of all components"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"deliverables",children:"Deliverables"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Complete ROS 2 package with all components"}),"\n",(0,t.jsx)(n.li,{children:"Demonstration video (90 seconds max)"}),"\n",(0,t.jsx)(n.li,{children:"Documentation and code comments"}),"\n",(0,t.jsx)(n.li,{children:"GitHub repository with setup instructions"}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"-content-creation-workflow",children:"\ud83d\udcdd Content Creation Workflow"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Created Using"}),": SpecKit Plus workflow via Claude Code"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Spec Phase"}),": ",(0,t.jsx)(n.code,{children:"/sp.specify"})," - Defined VLA module and Capstone requirements"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Plan Phase"}),": ",(0,t.jsx)(n.code,{children:"/sp.plan"})," - Structured 5 lessons with learning objectives"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Tasks Phase"}),": ",(0,t.jsx)(n.code,{children:"/sp.tasks"})," - Created implementation checklist"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Implement Phase"}),": ",(0,t.jsx)(n.code,{children:"/sp.implement"})," - Created content using ",(0,t.jsx)(n.code,{children:"physical-ai-content-writer"})," subagent"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Constitution Compliance"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"\u2705 4-Layer Teaching Method (Foundation \u2192 Application \u2192 Integration \u2192 Innovation)"}),"\n",(0,t.jsx)(n.li,{children:"\u2705 AI Three Roles Framework (Teacher, Copilot, Evaluator)"}),"\n",(0,t.jsx)(n.li,{children:"\u2705 CEFR Cognitive Load Management (B2-B2+)"}),"\n",(0,t.jsx)(n.li,{children:"\u2705 SpecKit Plus Workflow (spec\u2192plan\u2192tasks\u2192implement)"}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Ready to start?"})," Begin with ",(0,t.jsx)(n.a,{href:"/physical-ai-book/ur/docs/chapter-07/lesson-01-whisper-voice",children:"Lesson 7.1: Voice-to-Action with OpenAI Whisper"}),"!"]})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}}}]);