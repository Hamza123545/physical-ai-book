if(void 0===__webpack_require__)var __webpack_require__={};(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[447],{8453:(e,n,i)=>{i.d(n,{R:()=>s,x:()=>a});var t=i(6540);const r={},o=t.createContext(r);function s(e){const n=t.useContext(o);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),t.createElement(o.Provider,{value:n},e.children)}},9647:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>p,frontMatter:()=>s,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"chapter-04/lesson-05-multi-agent-rl","title":"Lesson 4.5: Multi-Agent Reinforcement Learning","description":"Coordinate multiple robots using multi-agent RL","source":"@site/i18n/ur/docusaurus-plugin-content-docs/current/chapter-04/lesson-05-multi-agent-rl.md","sourceDirName":"chapter-04","slug":"/chapter-04/lesson-05-multi-agent-rl","permalink":"/physical-ai-book/ur/docs/chapter-04/lesson-05-multi-agent-rl","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"multi-agent","permalink":"/physical-ai-book/ur/docs/tags/multi-agent"},{"inline":true,"label":"cooperation","permalink":"/physical-ai-book/ur/docs/tags/cooperation"},{"inline":true,"label":"communication","permalink":"/physical-ai-book/ur/docs/tags/communication"},{"inline":true,"label":"coordination","permalink":"/physical-ai-book/ur/docs/tags/coordination"}],"version":"current","frontMatter":{"title":"Lesson 4.5: Multi-Agent Reinforcement Learning","description":"Coordinate multiple robots using multi-agent RL","chapter":4,"lesson":5,"estimated_time":60,"cefr_level":"B2","blooms_level":"Apply","digcomp_level":6,"generated_by":"claude-sonnet-4-5-20250929","source_spec":"specs/002-physical-ai-textbook/spec.md","created":"2025-11-29","last_modified":"2025-11-29","git_author":"hswat","workflow":"/sp.implement","version":"1.0","prerequisites":["chapter-04-lesson-02"],"has_interactive_python":true,"interactive_python_count":3,"has_try_with_ai":true,"try_with_ai_count":2,"tags":["multi-agent","cooperation","communication","coordination"]},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 4.4: Policy Gradient Methods","permalink":"/physical-ai-book/ur/docs/chapter-04/lesson-04-policy-gradients"},"next":{"title":"Lesson 4.6: NVIDIA Isaac Sim - Photorealistic Simulation & Synthetic Data","permalink":"/physical-ai-book/ur/docs/chapter-04/lesson-06-isaac-sim"}}');var r=i(4848),o=i(8453);const s={title:"Lesson 4.5: Multi-Agent Reinforcement Learning",description:"Coordinate multiple robots using multi-agent RL",chapter:4,lesson:5,estimated_time:60,cefr_level:"B2",blooms_level:"Apply",digcomp_level:6,generated_by:"claude-sonnet-4-5-20250929",source_spec:"specs/002-physical-ai-textbook/spec.md",created:"2025-11-29",last_modified:"2025-11-29",git_author:"hswat",workflow:"/sp.implement",version:"1.0",prerequisites:["chapter-04-lesson-02"],has_interactive_python:!0,interactive_python_count:3,has_try_with_ai:!0,try_with_ai_count:2,tags:["multi-agent","cooperation","communication","coordination"]},a="Lesson 4.5: Multi-Agent Reinforcement Learning",l={},c=[{value:"\ud83c\udfaf Learning Objectives",id:"-learning-objectives",level:2},{value:"Introduction",id:"introduction",level:2},{value:"1. Independent Q-Learning",id:"1-independent-q-learning",level:2},{value:"2. Centralized Training, Decentralized Execution (CTDE)",id:"2-centralized-training-decentralized-execution-ctde",level:2},{value:"3. Communication",id:"3-communication",level:2},{value:"Exercises",id:"exercises",level:2},{value:"Exercise 4.5.1: Multi-Agent Environment",id:"exercise-451-multi-agent-environment",level:3},{value:"Exercise 4.5.2: Independent Q-Learning",id:"exercise-452-independent-q-learning",level:3},{value:"Exercise 4.5.3: Cooperative Reward Shaping",id:"exercise-453-cooperative-reward-shaping",level:3},{value:"Try With AI",id:"try-with-ai",level:2},{value:"TryWithAI 4.5.1: Multi-Agent Coordination",id:"trywithai-451-multi-agent-coordination",level:3},{value:"TryWithAI 4.5.2: Non-Stationarity Problem",id:"trywithai-452-non-stationarity-problem",level:3},{value:"Summary",id:"summary",level:2}];function d(e){const n={a:"a",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,o.R)(),...e.components},{InteractivePython:i,LearningObjectives:t,TryWithAI:s}=n;return i||h("InteractivePython",!0),t||h("LearningObjectives",!0),s||h("TryWithAI",!0),(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"lesson-45-multi-agent-reinforcement-learning",children:"Lesson 4.5: Multi-Agent Reinforcement Learning"})}),"\n",(0,r.jsx)(n.h2,{id:"-learning-objectives",children:"\ud83c\udfaf Learning Objectives"}),"\n",(0,r.jsx)(t,{cefr_level:"B2",objectives:[{text:"Implement independent Q-learning for multiple agents",blooms_level:"Apply",assessment_method:"Multi-agent exercise"},{text:"Design reward functions for cooperation",blooms_level:"Apply",assessment_method:"Reward shaping exercise"},{text:"Handle non-stationarity in multi-agent settings",blooms_level:"Understand",assessment_method:"TryWithAI"}]}),"\n",(0,r.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Multi-Agent RL"}),": Multiple robots learning simultaneously."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Challenges"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Non-stationary environment (other agents learning too)"}),"\n",(0,r.jsx)(n.li,{children:"Credit assignment (who caused success/failure?)"}),"\n",(0,r.jsx)(n.li,{children:"Communication and coordination"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"1-independent-q-learning",children:"1. Independent Q-Learning"}),"\n",(0,r.jsx)(n.p,{children:"Each agent learns independently. Simple but ignores other agents."}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"2-centralized-training-decentralized-execution-ctde",children:"2. Centralized Training, Decentralized Execution (CTDE)"}),"\n",(0,r.jsx)(n.p,{children:"Train with global info, execute with local observations."}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"3-communication",children:"3. Communication"}),"\n",(0,r.jsx)(n.p,{children:"Agents share observations or intentions."}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,r.jsx)(n.h3,{id:"exercise-451-multi-agent-environment",children:"Exercise 4.5.1: Multi-Agent Environment"}),"\n",(0,r.jsx)(i,{id:"ex-4-5-1",title:"Multi-Robot GridWorld",starterCode:'import numpy as np\n\nclass MultiAgentGrid:\n  def __init__(self, n_agents=2, grid_size=5):\n      self.n_agents = n_agents\n      self.size = grid_size\n      self.positions = [(0, 0), (grid_size-1, grid_size-1)]\n      self.goal = (2, 2)  # Shared goal\n\n  def step(self, actions):\n      """Take actions for all agents."""\n      # TODO: Move each agent, compute rewards\n      # Reward: +10 if all at goal, -0.1 per step\n      pass\n\n# Test\nenv = MultiAgentGrid(n_agents=2, grid_size=5)\nprint(f"Agents: {env.n_agents}, Goal: {env.goal}")\n',hints:["rewards = []","for i, action in enumerate(actions):","  self.positions[i] = self.move(self.positions[i], action)","  if self.positions[i] == self.goal:","    rewards.append(10)","  else:","    rewards.append(-0.1)","done = all(pos == self.goal for pos in self.positions)","return self.positions, rewards, done"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"exercise-452-independent-q-learning",children:"Exercise 4.5.2: Independent Q-Learning"}),"\n",(0,r.jsx)(i,{id:"ex-4-5-2",title:"Train Multiple Agents",starterCode:'import numpy as np\n\ndef train_multi_agent(env, n_agents, episodes=500):\n  """Train multiple agents with independent Q-learning."""\n  Q_tables = [np.zeros((env.n_states, env.n_actions)) for _ in range(n_agents)]\n\n  # TODO: For each episode:\n  #   Each agent selects action independently\n  #   Update each Q-table independently\n\n  return Q_tables\n\n# Placeholder\nprint("Multi-agent training defined")\n',hints:["for episode in range(episodes):","  states = env.reset()","  while not done:","    actions = [epsilon_greedy(Q_tables[i], states[i]) for i in range(n_agents)]","    next_states, rewards, done = env.step(actions)","    for i in range(n_agents):","      q_update(Q_tables[i], states[i], actions[i], rewards[i], next_states[i])","    states = next_states"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"exercise-453-cooperative-reward-shaping",children:"Exercise 4.5.3: Cooperative Reward Shaping"}),"\n",(0,r.jsx)(i,{id:"ex-4-5-3",title:"Design Cooperative Rewards",starterCode:'import numpy as np\n\ndef compute_cooperative_reward(positions, goal, actions):\n  """Compute reward encouraging cooperation."""\n  # TODO: Reward closer agents, penalize collisions\n  # Bonus if agents coordinate to reach goal together\n  pass\n\n# Test\npositions = [(1, 1), (3, 3)]\ngoal = (2, 2)\nreward = compute_cooperative_reward(positions, goal, [0, 1])\nprint(f"Cooperative reward: {reward}")\n',hints:["distances = [np.linalg.norm(np.array(pos) - np.array(goal)) for pos in positions]","reward = -np.mean(distances)  # Closer is better","if positions[0] == positions[1]:  # Collision","  reward -= 10","if all(pos == goal for pos in positions):  # All at goal","  reward += 50","return reward"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"try-with-ai",children:"Try With AI"}),"\n",(0,r.jsx)(n.h3,{id:"trywithai-451-multi-agent-coordination",children:"TryWithAI 4.5.1: Multi-Agent Coordination"}),"\n",(0,r.jsx)(s,{id:"tryai-4-5-1",title:"Improve Coordination",role:"Copilot",scenario:"Your agents don't cooperate\u2014they interfere with each other.",yourTask:"Implement Exercise 4.5.2. Observe agents blocking each other or racing to goal.",aiPromptTemplate:"My multi-agent system has agents interfering. They block paths or compete instead of cooperate. Here's my code: [paste]. Can you suggest: (1) Better reward shaping? (2) Communication mechanisms? (3) Should I use centralized training?",successCriteria:["You understand coordination challenges","You know reward shaping for cooperation","You've heard of CTDE approaches"],reflectionQuestions:["How do you measure cooperation?","What's emergent behavior?","When is communication essential?"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"trywithai-452-non-stationarity-problem",children:"TryWithAI 4.5.2: Non-Stationarity Problem"}),"\n",(0,r.jsx)(s,{id:"tryai-4-5-2",title:"Handle Non-Stationary Environments",role:"Evaluator",scenario:"As agents learn, environment becomes non-stationary (other agents' policies change).",yourTask:"Train agents with Exercise 4.5.2. Notice performance oscillates.",aiPromptTemplate:"My multi-agent training is unstable. Agent 1's performance improves then degrades as Agent 2 learns. Here's my training curve: [describe]. Can you review: (1) Why does this happen? (2) What's the non-stationarity problem? (3) Solutions like opponent modeling or parameter sharing?",successCriteria:["You understand non-stationarity in MARL","You know stabilization techniques","You've explored opponent modeling"],reflectionQuestions:["Can single-agent RL theory apply to MARL?","What's Nash equilibrium in MARL?","How to ensure convergence?"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Multi-Agent RL"}),": Multiple learning agents interacting"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Independent Q-Learning"}),": Simple but treats others as environment"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Cooperation"}),": Reward shaping, communication, CTDE"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"What's Next"}),": ",(0,r.jsx)(n.a,{href:"/physical-ai-book/ur/docs/chapter-04/quiz",children:"Chapter 4 Quiz"})]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Estimated time"}),": 60 minutes | ",(0,r.jsx)(n.strong,{children:"Difficulty"}),": B2"]})]})}function p(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}function h(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}}}]);