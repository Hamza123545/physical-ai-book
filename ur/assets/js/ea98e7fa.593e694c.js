if(void 0===__webpack_require__)var __webpack_require__={};(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[2233],{5053:(e,s,a)=>{a.r(s),a.d(s,{assets:()=>c,contentTitle:()=>l,default:()=>o,frontMatter:()=>i,metadata:()=>n,toc:()=>m});const n=JSON.parse('{"id":"chapter-04/lesson-03-deep-q-networks","title":"Lesson 4.3: Deep Q-Networks (DQN)","description":"Scale Q-learning to high-dimensional states with neural networks","source":"@site/i18n/ur/docusaurus-plugin-content-docs/current/chapter-04/lesson-03-deep-q-networks.md","sourceDirName":"chapter-04","slug":"/chapter-04/lesson-03-deep-q-networks","permalink":"/physical-ai-book/ur/docs/chapter-04/lesson-03-deep-q-networks","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"dqn","permalink":"/physical-ai-book/ur/docs/tags/dqn"},{"inline":true,"label":"deep-learning","permalink":"/physical-ai-book/ur/docs/tags/deep-learning"},{"inline":true,"label":"experience-replay","permalink":"/physical-ai-book/ur/docs/tags/experience-replay"},{"inline":true,"label":"target-network","permalink":"/physical-ai-book/ur/docs/tags/target-network"}],"version":"current","frontMatter":{"title":"Lesson 4.3: Deep Q-Networks (DQN)","description":"Scale Q-learning to high-dimensional states with neural networks","chapter":4,"lesson":3,"estimated_time":70,"cefr_level":"B2","blooms_level":"Apply","digcomp_level":6,"generated_by":"claude-sonnet-4-5-20250929","source_spec":"specs/002-physical-ai-textbook/spec.md","created":"2025-11-29","last_modified":"2025-11-29","git_author":"hswat","workflow":"/sp.implement","version":"1.0","prerequisites":["chapter-04-lesson-02"],"has_interactive_python":true,"interactive_python_count":4,"has_try_with_ai":true,"try_with_ai_count":2,"tags":["dqn","deep-learning","experience-replay","target-network"]},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 4.2: Q-Learning Algorithm","permalink":"/physical-ai-book/ur/docs/chapter-04/lesson-02-q-learning"},"next":{"title":"Lesson 4.4: Policy Gradient Methods","permalink":"/physical-ai-book/ur/docs/chapter-04/lesson-04-policy-gradients"}}');var t=a(4848),r=a(8453);const i={title:"Lesson 4.3: Deep Q-Networks (DQN)",description:"Scale Q-learning to high-dimensional states with neural networks",chapter:4,lesson:3,estimated_time:70,cefr_level:"B2",blooms_level:"Apply",digcomp_level:6,generated_by:"claude-sonnet-4-5-20250929",source_spec:"specs/002-physical-ai-textbook/spec.md",created:"2025-11-29",last_modified:"2025-11-29",git_author:"hswat",workflow:"/sp.implement",version:"1.0",prerequisites:["chapter-04-lesson-02"],has_interactive_python:!0,interactive_python_count:4,has_try_with_ai:!0,try_with_ai_count:2,tags:["dqn","deep-learning","experience-replay","target-network"]},l="Lesson 4.3: Deep Q-Networks (DQN)",c={},m=[{value:"\ud83c\udfaf Learning Objectives",id:"-learning-objectives",level:2},{value:"Introduction",id:"introduction",level:2},{value:"1. Neural Q-Network",id:"1-neural-q-network",level:2},{value:"2. Experience Replay",id:"2-experience-replay",level:2},{value:"3. Target Network",id:"3-target-network",level:2},{value:"Exercises",id:"exercises",level:2},{value:"Exercise 4.3.1: Q-Network Architecture",id:"exercise-431-q-network-architecture",level:3},{value:"Exercise 4.3.2: Experience Replay Buffer",id:"exercise-432-experience-replay-buffer",level:3},{value:"Exercise 4.3.3: DQN Update",id:"exercise-433-dqn-update",level:3},{value:"Exercise 4.3.4: Train DQN Agent",id:"exercise-434-train-dqn-agent",level:3},{value:"Try With AI",id:"try-with-ai",level:2},{value:"TryWithAI 4.3.1: DQN Architecture Design",id:"trywithai-431-dqn-architecture-design",level:3},{value:"TryWithAI 4.3.2: Debugging DQN",id:"trywithai-432-debugging-dqn",level:3},{value:"Summary",id:"summary",level:2}];function h(e){const s={a:"a",annotation:"annotation",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",math:"math",mi:"mi",mo:"mo",mrow:"mrow",msub:"msub",msup:"msup",ol:"ol",p:"p",semantics:"semantics",span:"span",strong:"strong",ul:"ul",...(0,r.R)(),...e.components},{InteractivePython:a,LearningObjectives:n,TryWithAI:i}=s;return a||d("InteractivePython",!0),n||d("LearningObjectives",!0),i||d("TryWithAI",!0),(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(s.header,{children:(0,t.jsx)(s.h1,{id:"lesson-43-deep-q-networks-dqn",children:"Lesson 4.3: Deep Q-Networks (DQN)"})}),"\n",(0,t.jsx)(s.h2,{id:"-learning-objectives",children:"\ud83c\udfaf Learning Objectives"}),"\n",(0,t.jsx)(n,{cefr_level:"B2",objectives:[{text:"Implement neural network Q-function approximator",blooms_level:"Apply",assessment_method:"Network exercise"},{text:"Apply experience replay for stable learning",blooms_level:"Apply",assessment_method:"Replay buffer exercise"},{text:"Use target network to reduce overestimation",blooms_level:"Apply",assessment_method:"DQN exercise"}]}),"\n",(0,t.jsx)(s.h2,{id:"introduction",children:"Introduction"}),"\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Problem"}),": Q-tables don't scale to large/continuous state spaces."]}),"\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Solution"}),": Approximate ",(0,t.jsxs)(s.span,{className:"katex",children:[(0,t.jsx)(s.span,{className:"katex-mathml",children:(0,t.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,t.jsxs)(s.semantics,{children:[(0,t.jsxs)(s.mrow,{children:[(0,t.jsx)(s.mi,{children:"Q"}),(0,t.jsx)(s.mo,{stretchy:"false",children:"("}),(0,t.jsx)(s.mi,{children:"s"}),(0,t.jsx)(s.mo,{separator:"true",children:","}),(0,t.jsx)(s.mi,{children:"a"}),(0,t.jsx)(s.mo,{stretchy:"false",children:")"})]}),(0,t.jsx)(s.annotation,{encoding:"application/x-tex",children:"Q(s,a)"})]})})}),(0,t.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,t.jsxs)(s.span,{className:"base",children:[(0,t.jsx)(s.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,t.jsx)(s.span,{className:"mord mathnormal",children:"Q"}),(0,t.jsx)(s.span,{className:"mopen",children:"("}),(0,t.jsx)(s.span,{className:"mord mathnormal",children:"s"}),(0,t.jsx)(s.span,{className:"mpunct",children:","}),(0,t.jsx)(s.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,t.jsx)(s.span,{className:"mord mathnormal",children:"a"}),(0,t.jsx)(s.span,{className:"mclose",children:")"})]})})]})," with neural network ",(0,t.jsxs)(s.span,{className:"katex",children:[(0,t.jsx)(s.span,{className:"katex-mathml",children:(0,t.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,t.jsxs)(s.semantics,{children:[(0,t.jsxs)(s.mrow,{children:[(0,t.jsx)(s.mi,{children:"Q"}),(0,t.jsx)(s.mo,{stretchy:"false",children:"("}),(0,t.jsx)(s.mi,{children:"s"}),(0,t.jsx)(s.mo,{separator:"true",children:","}),(0,t.jsx)(s.mi,{children:"a"}),(0,t.jsx)(s.mo,{separator:"true",children:";"}),(0,t.jsx)(s.mi,{children:"\u03b8"}),(0,t.jsx)(s.mo,{stretchy:"false",children:")"})]}),(0,t.jsx)(s.annotation,{encoding:"application/x-tex",children:"Q(s,a; \\theta)"})]})})}),(0,t.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,t.jsxs)(s.span,{className:"base",children:[(0,t.jsx)(s.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,t.jsx)(s.span,{className:"mord mathnormal",children:"Q"}),(0,t.jsx)(s.span,{className:"mopen",children:"("}),(0,t.jsx)(s.span,{className:"mord mathnormal",children:"s"}),(0,t.jsx)(s.span,{className:"mpunct",children:","}),(0,t.jsx)(s.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,t.jsx)(s.span,{className:"mord mathnormal",children:"a"}),(0,t.jsx)(s.span,{className:"mpunct",children:";"}),(0,t.jsx)(s.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,t.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.02778em"},children:"\u03b8"}),(0,t.jsx)(s.span,{className:"mclose",children:")"})]})})]}),"."]}),"\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"DQN innovations"}),":"]}),"\n",(0,t.jsxs)(s.ol,{children:["\n",(0,t.jsx)(s.li,{children:"Experience replay"}),"\n",(0,t.jsx)(s.li,{children:"Target network"}),"\n"]}),"\n",(0,t.jsx)(s.hr,{}),"\n",(0,t.jsx)(s.h2,{id:"1-neural-q-network",children:"1. Neural Q-Network"}),"\n",(0,t.jsx)(s.p,{children:"Replace Q-table with neural network:"}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsxs)(s.li,{children:["Input: state ",(0,t.jsxs)(s.span,{className:"katex",children:[(0,t.jsx)(s.span,{className:"katex-mathml",children:(0,t.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,t.jsxs)(s.semantics,{children:[(0,t.jsx)(s.mrow,{children:(0,t.jsx)(s.mi,{children:"s"})}),(0,t.jsx)(s.annotation,{encoding:"application/x-tex",children:"s"})]})})}),(0,t.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,t.jsxs)(s.span,{className:"base",children:[(0,t.jsx)(s.span,{className:"strut",style:{height:"0.4306em"}}),(0,t.jsx)(s.span,{className:"mord mathnormal",children:"s"})]})})]})]}),"\n",(0,t.jsx)(s.li,{children:"Output: Q-values for all actions"}),"\n"]}),"\n",(0,t.jsx)(s.hr,{}),"\n",(0,t.jsx)(s.h2,{id:"2-experience-replay",children:"2. Experience Replay"}),"\n",(0,t.jsxs)(s.p,{children:["Store transitions ",(0,t.jsxs)(s.span,{className:"katex",children:[(0,t.jsx)(s.span,{className:"katex-mathml",children:(0,t.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,t.jsxs)(s.semantics,{children:[(0,t.jsxs)(s.mrow,{children:[(0,t.jsx)(s.mo,{stretchy:"false",children:"("}),(0,t.jsx)(s.mi,{children:"s"}),(0,t.jsx)(s.mo,{separator:"true",children:","}),(0,t.jsx)(s.mi,{children:"a"}),(0,t.jsx)(s.mo,{separator:"true",children:","}),(0,t.jsx)(s.mi,{children:"r"}),(0,t.jsx)(s.mo,{separator:"true",children:","}),(0,t.jsxs)(s.msup,{children:[(0,t.jsx)(s.mi,{children:"s"}),(0,t.jsx)(s.mo,{mathvariant:"normal",lspace:"0em",rspace:"0em",children:"\u2032"})]}),(0,t.jsx)(s.mo,{stretchy:"false",children:")"})]}),(0,t.jsx)(s.annotation,{encoding:"application/x-tex",children:"(s, a, r, s')"})]})})}),(0,t.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,t.jsxs)(s.span,{className:"base",children:[(0,t.jsx)(s.span,{className:"strut",style:{height:"1.0019em",verticalAlign:"-0.25em"}}),(0,t.jsx)(s.span,{className:"mopen",children:"("}),(0,t.jsx)(s.span,{className:"mord mathnormal",children:"s"}),(0,t.jsx)(s.span,{className:"mpunct",children:","}),(0,t.jsx)(s.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,t.jsx)(s.span,{className:"mord mathnormal",children:"a"}),(0,t.jsx)(s.span,{className:"mpunct",children:","}),(0,t.jsx)(s.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,t.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.02778em"},children:"r"}),(0,t.jsx)(s.span,{className:"mpunct",children:","}),(0,t.jsx)(s.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,t.jsxs)(s.span,{className:"mord",children:[(0,t.jsx)(s.span,{className:"mord mathnormal",children:"s"}),(0,t.jsx)(s.span,{className:"msupsub",children:(0,t.jsx)(s.span,{className:"vlist-t",children:(0,t.jsx)(s.span,{className:"vlist-r",children:(0,t.jsx)(s.span,{className:"vlist",style:{height:"0.7519em"},children:(0,t.jsxs)(s.span,{style:{top:"-3.063em",marginRight:"0.05em"},children:[(0,t.jsx)(s.span,{className:"pstrut",style:{height:"2.7em"}}),(0,t.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,t.jsx)(s.span,{className:"mord mtight",children:(0,t.jsx)(s.span,{className:"mord mtight",children:"\u2032"})})})]})})})})})]}),(0,t.jsx)(s.span,{className:"mclose",children:")"})]})})]})," in replay buffer. Sample random minibatch for training."]}),"\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Why?"})," Breaks correlation, stabilizes learning."]}),"\n",(0,t.jsx)(s.hr,{}),"\n",(0,t.jsx)(s.h2,{id:"3-target-network",children:"3. Target Network"}),"\n",(0,t.jsxs)(s.p,{children:["Use separate target network ",(0,t.jsxs)(s.span,{className:"katex",children:[(0,t.jsx)(s.span,{className:"katex-mathml",children:(0,t.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,t.jsxs)(s.semantics,{children:[(0,t.jsxs)(s.mrow,{children:[(0,t.jsx)(s.mi,{children:"Q"}),(0,t.jsx)(s.mo,{stretchy:"false",children:"("}),(0,t.jsx)(s.mi,{children:"s"}),(0,t.jsx)(s.mo,{separator:"true",children:","}),(0,t.jsx)(s.mi,{children:"a"}),(0,t.jsx)(s.mo,{separator:"true",children:";"}),(0,t.jsxs)(s.msup,{children:[(0,t.jsx)(s.mi,{children:"\u03b8"}),(0,t.jsx)(s.mo,{children:"\u2212"})]}),(0,t.jsx)(s.mo,{stretchy:"false",children:")"})]}),(0,t.jsx)(s.annotation,{encoding:"application/x-tex",children:"Q(s,a; \\theta^-)"})]})})}),(0,t.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,t.jsxs)(s.span,{className:"base",children:[(0,t.jsx)(s.span,{className:"strut",style:{height:"1.0213em",verticalAlign:"-0.25em"}}),(0,t.jsx)(s.span,{className:"mord mathnormal",children:"Q"}),(0,t.jsx)(s.span,{className:"mopen",children:"("}),(0,t.jsx)(s.span,{className:"mord mathnormal",children:"s"}),(0,t.jsx)(s.span,{className:"mpunct",children:","}),(0,t.jsx)(s.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,t.jsx)(s.span,{className:"mord mathnormal",children:"a"}),(0,t.jsx)(s.span,{className:"mpunct",children:";"}),(0,t.jsx)(s.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,t.jsxs)(s.span,{className:"mord",children:[(0,t.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.02778em"},children:"\u03b8"}),(0,t.jsx)(s.span,{className:"msupsub",children:(0,t.jsx)(s.span,{className:"vlist-t",children:(0,t.jsx)(s.span,{className:"vlist-r",children:(0,t.jsx)(s.span,{className:"vlist",style:{height:"0.7713em"},children:(0,t.jsxs)(s.span,{style:{top:"-3.063em",marginRight:"0.05em"},children:[(0,t.jsx)(s.span,{className:"pstrut",style:{height:"2.7em"}}),(0,t.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,t.jsx)(s.span,{className:"mbin mtight",children:"\u2212"})})]})})})})})]}),(0,t.jsx)(s.span,{className:"mclose",children:")"})]})})]})," for TD target:\n",(0,t.jsxs)(s.span,{className:"katex",children:[(0,t.jsx)(s.span,{className:"katex-mathml",children:(0,t.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,t.jsxs)(s.semantics,{children:[(0,t.jsxs)(s.mrow,{children:[(0,t.jsx)(s.mi,{children:"y"}),(0,t.jsx)(s.mo,{children:"="}),(0,t.jsx)(s.mi,{children:"r"}),(0,t.jsx)(s.mo,{children:"+"}),(0,t.jsx)(s.mi,{children:"\u03b3"}),(0,t.jsxs)(s.msub,{children:[(0,t.jsxs)(s.mrow,{children:[(0,t.jsx)(s.mi,{children:"max"}),(0,t.jsx)(s.mo,{children:"\u2061"})]}),(0,t.jsxs)(s.msup,{children:[(0,t.jsx)(s.mi,{children:"a"}),(0,t.jsx)(s.mo,{mathvariant:"normal",lspace:"0em",rspace:"0em",children:"\u2032"})]})]}),(0,t.jsx)(s.mi,{children:"Q"}),(0,t.jsx)(s.mo,{stretchy:"false",children:"("}),(0,t.jsxs)(s.msup,{children:[(0,t.jsx)(s.mi,{children:"s"}),(0,t.jsx)(s.mo,{mathvariant:"normal",lspace:"0em",rspace:"0em",children:"\u2032"})]}),(0,t.jsx)(s.mo,{separator:"true",children:","}),(0,t.jsxs)(s.msup,{children:[(0,t.jsx)(s.mi,{children:"a"}),(0,t.jsx)(s.mo,{mathvariant:"normal",lspace:"0em",rspace:"0em",children:"\u2032"})]}),(0,t.jsx)(s.mo,{separator:"true",children:";"}),(0,t.jsxs)(s.msup,{children:[(0,t.jsx)(s.mi,{children:"\u03b8"}),(0,t.jsx)(s.mo,{children:"\u2212"})]}),(0,t.jsx)(s.mo,{stretchy:"false",children:")"})]}),(0,t.jsx)(s.annotation,{encoding:"application/x-tex",children:"y = r + \\gamma \\max_{a'} Q(s', a'; \\theta^-)"})]})})}),(0,t.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,t.jsxs)(s.span,{className:"base",children:[(0,t.jsx)(s.span,{className:"strut",style:{height:"0.625em",verticalAlign:"-0.1944em"}}),(0,t.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.03588em"},children:"y"}),(0,t.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}}),(0,t.jsx)(s.span,{className:"mrel",children:"="}),(0,t.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}})]}),(0,t.jsxs)(s.span,{className:"base",children:[(0,t.jsx)(s.span,{className:"strut",style:{height:"0.6667em",verticalAlign:"-0.0833em"}}),(0,t.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.02778em"},children:"r"}),(0,t.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,t.jsx)(s.span,{className:"mbin",children:"+"}),(0,t.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,t.jsxs)(s.span,{className:"base",children:[(0,t.jsx)(s.span,{className:"strut",style:{height:"1.0213em",verticalAlign:"-0.25em"}}),(0,t.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.05556em"},children:"\u03b3"}),(0,t.jsx)(s.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,t.jsxs)(s.span,{className:"mop",children:[(0,t.jsx)(s.span,{className:"mop",children:"max"}),(0,t.jsx)(s.span,{className:"msupsub",children:(0,t.jsxs)(s.span,{className:"vlist-t vlist-t2",children:[(0,t.jsxs)(s.span,{className:"vlist-r",children:[(0,t.jsx)(s.span,{className:"vlist",style:{height:"0.328em"},children:(0,t.jsxs)(s.span,{style:{top:"-2.55em",marginRight:"0.05em"},children:[(0,t.jsx)(s.span,{className:"pstrut",style:{height:"2.7em"}}),(0,t.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,t.jsx)(s.span,{className:"mord mtight",children:(0,t.jsxs)(s.span,{className:"mord mtight",children:[(0,t.jsx)(s.span,{className:"mord mathnormal mtight",children:"a"}),(0,t.jsx)(s.span,{className:"msupsub",children:(0,t.jsx)(s.span,{className:"vlist-t",children:(0,t.jsx)(s.span,{className:"vlist-r",children:(0,t.jsx)(s.span,{className:"vlist",style:{height:"0.6828em"},children:(0,t.jsxs)(s.span,{style:{top:"-2.786em",marginRight:"0.0714em"},children:[(0,t.jsx)(s.span,{className:"pstrut",style:{height:"2.5em"}}),(0,t.jsx)(s.span,{className:"sizing reset-size3 size1 mtight",children:(0,t.jsx)(s.span,{className:"mord mtight",children:(0,t.jsx)(s.span,{className:"mord mtight",children:"\u2032"})})})]})})})})})]})})})]})}),(0,t.jsx)(s.span,{className:"vlist-s",children:"\u200b"})]}),(0,t.jsx)(s.span,{className:"vlist-r",children:(0,t.jsx)(s.span,{className:"vlist",style:{height:"0.15em"},children:(0,t.jsx)(s.span,{})})})]})})]}),(0,t.jsx)(s.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,t.jsx)(s.span,{className:"mord mathnormal",children:"Q"}),(0,t.jsx)(s.span,{className:"mopen",children:"("}),(0,t.jsxs)(s.span,{className:"mord",children:[(0,t.jsx)(s.span,{className:"mord mathnormal",children:"s"}),(0,t.jsx)(s.span,{className:"msupsub",children:(0,t.jsx)(s.span,{className:"vlist-t",children:(0,t.jsx)(s.span,{className:"vlist-r",children:(0,t.jsx)(s.span,{className:"vlist",style:{height:"0.7519em"},children:(0,t.jsxs)(s.span,{style:{top:"-3.063em",marginRight:"0.05em"},children:[(0,t.jsx)(s.span,{className:"pstrut",style:{height:"2.7em"}}),(0,t.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,t.jsx)(s.span,{className:"mord mtight",children:(0,t.jsx)(s.span,{className:"mord mtight",children:"\u2032"})})})]})})})})})]}),(0,t.jsx)(s.span,{className:"mpunct",children:","}),(0,t.jsx)(s.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,t.jsxs)(s.span,{className:"mord",children:[(0,t.jsx)(s.span,{className:"mord mathnormal",children:"a"}),(0,t.jsx)(s.span,{className:"msupsub",children:(0,t.jsx)(s.span,{className:"vlist-t",children:(0,t.jsx)(s.span,{className:"vlist-r",children:(0,t.jsx)(s.span,{className:"vlist",style:{height:"0.7519em"},children:(0,t.jsxs)(s.span,{style:{top:"-3.063em",marginRight:"0.05em"},children:[(0,t.jsx)(s.span,{className:"pstrut",style:{height:"2.7em"}}),(0,t.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,t.jsx)(s.span,{className:"mord mtight",children:(0,t.jsx)(s.span,{className:"mord mtight",children:"\u2032"})})})]})})})})})]}),(0,t.jsx)(s.span,{className:"mpunct",children:";"}),(0,t.jsx)(s.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,t.jsxs)(s.span,{className:"mord",children:[(0,t.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.02778em"},children:"\u03b8"}),(0,t.jsx)(s.span,{className:"msupsub",children:(0,t.jsx)(s.span,{className:"vlist-t",children:(0,t.jsx)(s.span,{className:"vlist-r",children:(0,t.jsx)(s.span,{className:"vlist",style:{height:"0.7713em"},children:(0,t.jsxs)(s.span,{style:{top:"-3.063em",marginRight:"0.05em"},children:[(0,t.jsx)(s.span,{className:"pstrut",style:{height:"2.7em"}}),(0,t.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,t.jsx)(s.span,{className:"mbin mtight",children:"\u2212"})})]})})})})})]}),(0,t.jsx)(s.span,{className:"mclose",children:")"})]})]})]})]}),"\n",(0,t.jsxs)(s.p,{children:["Update ",(0,t.jsxs)(s.span,{className:"katex",children:[(0,t.jsx)(s.span,{className:"katex-mathml",children:(0,t.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,t.jsxs)(s.semantics,{children:[(0,t.jsx)(s.mrow,{children:(0,t.jsxs)(s.msup,{children:[(0,t.jsx)(s.mi,{children:"\u03b8"}),(0,t.jsx)(s.mo,{children:"\u2212"})]})}),(0,t.jsx)(s.annotation,{encoding:"application/x-tex",children:"\\theta^-"})]})})}),(0,t.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,t.jsxs)(s.span,{className:"base",children:[(0,t.jsx)(s.span,{className:"strut",style:{height:"0.7713em"}}),(0,t.jsxs)(s.span,{className:"mord",children:[(0,t.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.02778em"},children:"\u03b8"}),(0,t.jsx)(s.span,{className:"msupsub",children:(0,t.jsx)(s.span,{className:"vlist-t",children:(0,t.jsx)(s.span,{className:"vlist-r",children:(0,t.jsx)(s.span,{className:"vlist",style:{height:"0.7713em"},children:(0,t.jsxs)(s.span,{style:{top:"-3.063em",marginRight:"0.05em"},children:[(0,t.jsx)(s.span,{className:"pstrut",style:{height:"2.7em"}}),(0,t.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,t.jsx)(s.span,{className:"mbin mtight",children:"\u2212"})})]})})})})})]})]})})]})," every N steps."]}),"\n",(0,t.jsx)(s.hr,{}),"\n",(0,t.jsx)(s.h2,{id:"exercises",children:"Exercises"}),"\n",(0,t.jsx)(s.h3,{id:"exercise-431-q-network-architecture",children:"Exercise 4.3.1: Q-Network Architecture"}),"\n",(0,t.jsx)(a,{id:"ex-4-3-1",title:"Build Q-Network (Simplified)",starterCode:'import numpy as np\n\nclass QNetwork:\n  def __init__(self, state_dim, n_actions, hidden=64):\n      """Simple Q-network."""\n      # TODO: Initialize weights for 2-layer network\n      self.W1 = None\n      self.W2 = None\n\n  def forward(self, state):\n      """Forward pass: state -> Q-values."""\n      # TODO: h = relu(state @ W1)\n      #       q = h @ W2\n      pass\n\n# Test\nnet = QNetwork(state_dim=4, n_actions=2, hidden=64)\nq_values = net.forward(np.array([0.1, 0.2, 0.3, 0.4]))\nprint(f"Q-values shape: {q_values.shape if q_values is not None else \'None\'}")\n',hints:["self.W1 = np.random.randn(state_dim, hidden) * 0.01","self.W2 = np.random.randn(hidden, n_actions) * 0.01","h = np.maximum(0, state @ self.W1)  # ReLU","return h @ self.W2"]}),"\n",(0,t.jsx)(s.hr,{}),"\n",(0,t.jsx)(s.h3,{id:"exercise-432-experience-replay-buffer",children:"Exercise 4.3.2: Experience Replay Buffer"}),"\n",(0,t.jsx)(a,{id:"ex-4-3-2",title:"Replay Buffer",starterCode:'import numpy as np\nfrom collections import deque\n\nclass ReplayBuffer:\n  def __init__(self, capacity=10000):\n      self.buffer = deque(maxlen=capacity)\n\n  def add(self, state, action, reward, next_state, done):\n      """Store transition."""\n      # TODO: Append (s, a, r, s\', done) to buffer\n      pass\n\n  def sample(self, batch_size):\n      """Sample random minibatch."""\n      # TODO: Random sample batch_size transitions\n      # Return as separate arrays\n      pass\n\n# Test\nbuffer = ReplayBuffer(capacity=100)\nbuffer.add([0.1], 0, 1.0, [0.2], False)\nprint(f"Buffer size: {len(buffer.buffer)}")\n',hints:["self.buffer.append((state, action, reward, next_state, done))","indices = np.random.choice(len(self.buffer), batch_size)","batch = [self.buffer[i] for i in indices]","states, actions, rewards, next_states, dones = zip(*batch)","return np.array(states), np.array(actions), ..."]}),"\n",(0,t.jsx)(s.hr,{}),"\n",(0,t.jsx)(s.h3,{id:"exercise-433-dqn-update",children:"Exercise 4.3.3: DQN Update"}),"\n",(0,t.jsx)(a,{id:"ex-4-3-3",title:"DQN Training Step",starterCode:'import numpy as np\n\ndef dqn_update(q_net, target_net, batch, gamma=0.99):\n  """Compute DQN loss and gradient."""\n  states, actions, rewards, next_states, dones = batch\n\n  # TODO:\n  # 1. Compute target: y = r + gamma * max Q_target(s\', a\')\n  # 2. Compute current Q: Q(s, a)\n  # 3. Loss = (Q - y)^2\n\n  pass\n\n# Placeholder test\nprint("DQN update function defined")\n',hints:["next_q_values = target_net.forward(next_states)","max_next_q = np.max(next_q_values, axis=1)","targets = rewards + gamma * max_next_q * (1 - dones)","current_q = q_net.forward(states)[range(len(actions)), actions]","loss = np.mean((current_q - targets) ** 2)"]}),"\n",(0,t.jsx)(s.hr,{}),"\n",(0,t.jsx)(s.h3,{id:"exercise-434-train-dqn-agent",children:"Exercise 4.3.4: Train DQN Agent"}),"\n",(0,t.jsx)(a,{id:"ex-4-3-4",title:"Full DQN Training Loop",starterCode:'import numpy as np\n\ndef train_dqn(env, episodes=100, batch_size=32, target_update=10):\n  """Train DQN agent."""\n  q_net = QNetwork(env.state_dim, env.n_actions)\n  target_net = QNetwork(env.state_dim, env.n_actions)\n  buffer = ReplayBuffer(capacity=10000)\n\n  # TODO: Training loop\n  # 1. Collect experience\n  # 2. Sample from buffer\n  # 3. Update Q-network\n  # 4. Update target network every N episodes\n\n  return q_net\n\n# Placeholder\nprint("DQN training loop defined")\n',hints:["for episode in range(episodes):","  state = env.reset()","  while not done:","    action = epsilon_greedy(q_net.forward(state), epsilon)","    next_state, reward, done = env.step(action)","    buffer.add(state, action, reward, next_state, done)","    if len(buffer) > batch_size:","      batch = buffer.sample(batch_size)","      dqn_update(q_net, target_net, batch)","  if episode % target_update == 0:","    target_net = copy(q_net)"]}),"\n",(0,t.jsx)(s.hr,{}),"\n",(0,t.jsx)(s.h2,{id:"try-with-ai",children:"Try With AI"}),"\n",(0,t.jsx)(s.h3,{id:"trywithai-431-dqn-architecture-design",children:"TryWithAI 4.3.1: DQN Architecture Design"}),"\n",(0,t.jsx)(i,{id:"tryai-4-3-1",title:"Design Network for Complex States",role:"Copilot",scenario:"You need to scale DQN to image inputs (e.g., Atari games).",yourTask:"Implement Exercise 4.3.1. Consider how to handle 84x84 pixel images.",aiPromptTemplate:"My DQN uses fully-connected layers for state=[x, y, vx, vy]. How do I modify it for image inputs (84x84x4)? Should I use convolutions? Here's my current architecture: [paste]. Can you suggest: (1) CNN architecture for images? (2) When to use conv vs FC? (3) How many parameters is reasonable?",successCriteria:["You understand CNN for spatial inputs","You know FC for low-dim states","You can estimate parameter counts"],reflectionQuestions:["Why use grayscale images in Atari DQN?","What's frame stacking?","How does architecture affect training time?"]}),"\n",(0,t.jsx)(s.hr,{}),"\n",(0,t.jsx)(s.h3,{id:"trywithai-432-debugging-dqn",children:"TryWithAI 4.3.2: Debugging DQN"}),"\n",(0,t.jsx)(i,{id:"tryai-4-3-2",title:"Diagnose Training Issues",role:"Evaluator",scenario:"Your DQN doesn't learn or diverges (Q-values explode).",yourTask:"Complete Exercise 4.3.4. Monitor loss and Q-values during training.",aiPromptTemplate:"My DQN training is unstable. Loss starts at X, then explodes to Y after Z episodes. Q-values: [paste statistics]. Here's my code: [paste]. Can you review: (1) Is my learning rate too high? (2) Am I updating target network correctly? (3) Common DQN bugs?",successCriteria:["You can diagnose DQN training issues","You know stabilization techniques","You understand hyperparameter sensitivity"],reflectionQuestions:["What causes Q-value overestimation?","Why does replay buffer help?","How often to update target network?"]}),"\n",(0,t.jsx)(s.hr,{}),"\n",(0,t.jsx)(s.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsxs)(s.ol,{children:["\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"DQN"}),": Neural network Q-function for large state spaces"]}),"\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"Experience Replay"}),": Random sampling breaks correlation"]}),"\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"Target Network"}),": Stabilizes TD targets"]}),"\n"]}),"\n",(0,t.jsxs)(s.p,{children:[(0,t.jsx)(s.strong,{children:"Next"}),": ",(0,t.jsx)(s.a,{href:"/physical-ai-book/ur/docs/chapter-04/lesson-04-policy-gradients",children:"Lesson 4.4: Policy Gradients"})]})]})}function o(e={}){const{wrapper:s}={...(0,r.R)(),...e.components};return s?(0,t.jsx)(s,{...e,children:(0,t.jsx)(h,{...e})}):h(e)}function d(e,s){throw new Error("Expected "+(s?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}},8453:(e,s,a)=>{a.d(s,{R:()=>i,x:()=>l});var n=a(6540);const t={},r=n.createContext(t);function i(e){const s=n.useContext(r);return n.useMemo(function(){return"function"==typeof e?e(s):{...s,...e}},[s,e])}function l(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:i(e.components),n.createElement(r.Provider,{value:s},e.children)}}}]);