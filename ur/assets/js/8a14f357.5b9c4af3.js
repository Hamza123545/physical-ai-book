if(void 0===__webpack_require__)var __webpack_require__={};(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[9400],{1909:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>u,frontMatter:()=>r,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"chapter-07/lesson-04-multimodal","title":"Lesson 7.4: Multi-Modal Interaction","description":"Combine speech, vision, and gesture for natural human-robot interaction","source":"@site/i18n/ur/docusaurus-plugin-content-docs/current/chapter-07/lesson-04-multimodal.md","sourceDirName":"chapter-07","slug":"/chapter-07/lesson-04-multimodal","permalink":"/physical-ai-book/ur/docs/chapter-07/lesson-04-multimodal","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"multimodal","permalink":"/physical-ai-book/ur/docs/tags/multimodal"},{"inline":true,"label":"sensor-fusion","permalink":"/physical-ai-book/ur/docs/tags/sensor-fusion"},{"inline":true,"label":"human-robot-interaction","permalink":"/physical-ai-book/ur/docs/tags/human-robot-interaction"},{"inline":true,"label":"vla","permalink":"/physical-ai-book/ur/docs/tags/vla"}],"version":"current","frontMatter":{"title":"Lesson 7.4: Multi-Modal Interaction","description":"Combine speech, vision, and gesture for natural human-robot interaction","chapter":7,"lesson":4,"estimated_time":60,"cefr_level":"B2+","blooms_level":"Apply","digcomp_level":6,"generated_by":"claude-sonnet-4-5-20250929","source_spec":"specs/002-physical-ai-textbook/spec.md","created":"2025-11-29","last_modified":"2025-11-29","git_author":"hswat","workflow":"/sp.implement","version":"1.0","prerequisites":["chapter-07-lesson-03"],"has_interactive_python":true,"interactive_python_count":1,"has_try_with_ai":false,"try_with_ai_count":0,"tags":["multimodal","sensor-fusion","human-robot-interaction","vla"]},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 7.3: Natural Language to ROS 2 Actions","permalink":"/physical-ai-book/ur/docs/chapter-07/lesson-03-nl-to-ros2"},"next":{"title":"Lesson 7.5: Capstone Project - The Autonomous Humanoid","permalink":"/physical-ai-book/ur/docs/chapter-07/lesson-05-capstone"}}');var s=t(4848),o=t(8453);const r={title:"Lesson 7.4: Multi-Modal Interaction",description:"Combine speech, vision, and gesture for natural human-robot interaction",chapter:7,lesson:4,estimated_time:60,cefr_level:"B2+",blooms_level:"Apply",digcomp_level:6,generated_by:"claude-sonnet-4-5-20250929",source_spec:"specs/002-physical-ai-textbook/spec.md",created:"2025-11-29",last_modified:"2025-11-29",git_author:"hswat",workflow:"/sp.implement",version:"1.0",prerequisites:["chapter-07-lesson-03"],has_interactive_python:!0,interactive_python_count:1,has_try_with_ai:!1,try_with_ai_count:0,tags:["multimodal","sensor-fusion","human-robot-interaction","vla"]},a="Lesson 7.4: Multi-Modal Interaction",c={},l=[{value:"\ud83c\udfaf Learning Objectives",id:"-learning-objectives",level:2},{value:"Introduction",id:"introduction",level:2},{value:"1. Multi-Modal Architecture",id:"1-multi-modal-architecture",level:2},{value:"Sensor Fusion",id:"sensor-fusion",level:3},{value:"2. Speech + Vision",id:"2-speech--vision",level:2},{value:"Combining Voice and Visual Cues",id:"combining-voice-and-visual-cues",level:3},{value:"3. Gesture Recognition",id:"3-gesture-recognition",level:2},{value:"Skeleton-Based Gestures",id:"skeleton-based-gestures",level:3},{value:"4. Conversational Interfaces",id:"4-conversational-interfaces",level:2},{value:"Dialogue Management",id:"dialogue-management",level:3},{value:"5. Exercises",id:"5-exercises",level:2},{value:"Exercise 7.4.1: Multi-Modal Fusion",id:"exercise-741-multi-modal-fusion",level:3},{value:"Summary",id:"summary",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components},{InteractivePython:t}=n;return t||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("InteractivePython",!0),(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"lesson-74-multi-modal-interaction",children:"Lesson 7.4: Multi-Modal Interaction"})}),"\n",(0,s.jsx)(n.h2,{id:"-learning-objectives",children:"\ud83c\udfaf Learning Objectives"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Integrate multiple input modalities (speech, vision, gesture)"}),"\n",(0,s.jsx)(n.li,{children:"Fuse sensor data for robust perception"}),"\n",(0,s.jsx)(n.li,{children:"Design natural interaction patterns"}),"\n",(0,s.jsx)(n.li,{children:"Implement conversational robot interfaces"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Time"}),": 60 minutes"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Multi-modal interaction"})," combines multiple input channels (speech, vision, gesture) to create natural, intuitive human-robot interfaces. This enables more robust and user-friendly robot systems."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Benefits"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Redundancy: If one modality fails, others compensate"}),"\n",(0,s.jsx)(n.li,{children:"Natural: Mimics human-human interaction"}),"\n",(0,s.jsx)(n.li,{children:"Robust: Works in noisy or visually challenging environments"}),"\n",(0,s.jsx)(n.li,{children:"Accessible: Supports different user preferences"}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"1-multi-modal-architecture",children:"1. Multi-Modal Architecture"}),"\n",(0,s.jsx)(n.h3,{id:"sensor-fusion",children:"Sensor Fusion"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class MultiModalPerception:\n    def __init__(self):\n        self.voice_processor = VoiceProcessor()\n        self.vision_processor = VisionProcessor()\n        self.gesture_recognizer = GestureRecognizer()\n        \n    def process_input(self, audio, image, skeleton_data):\n        """Fuse multi-modal inputs."""\n        voice_command = self.voice_processor.transcribe(audio)\n        visual_objects = self.vision_processor.detect_objects(image)\n        gesture = self.gesture_recognizer.recognize(skeleton_data)\n        \n        # Fuse information\n        intent = self.fuse_modalities(voice_command, visual_objects, gesture)\n        return intent\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"2-speech--vision",children:"2. Speech + Vision"}),"\n",(0,s.jsx)(n.h3,{id:"combining-voice-and-visual-cues",children:"Combining Voice and Visual Cues"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'def interpret_command(self, voice_text, camera_image):\n    """Combine voice command with visual context."""\n    # Extract objects from voice\n    mentioned_objects = extract_objects(voice_text)\n    \n    # Detect objects in image\n    detected_objects = self.vision_processor.detect(camera_image)\n    \n    # Match mentioned objects to detected objects\n    target_object = match_object(mentioned_objects, detected_objects)\n    \n    # Generate action\n    if "pick up" in voice_text.lower():\n        return {"action": "pick", "object": target_object}\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"3-gesture-recognition",children:"3. Gesture Recognition"}),"\n",(0,s.jsx)(n.h3,{id:"skeleton-based-gestures",children:"Skeleton-Based Gestures"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'def recognize_gesture(self, skeleton_keypoints):\n    """Recognize gesture from skeleton data."""\n    # Extract hand positions\n    left_hand = skeleton_keypoints["left_wrist"]\n    right_hand = skeleton_keypoints["right_wrist"]\n    \n    # Classify gesture\n    if is_waving(left_hand, right_hand):\n        return "wave"\n    elif is_pointing(right_hand):\n        return "point"\n    elif is_thumbs_up(right_hand):\n        return "approval"\n    \n    return None\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"4-conversational-interfaces",children:"4. Conversational Interfaces"}),"\n",(0,s.jsx)(n.h3,{id:"dialogue-management",children:"Dialogue Management"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class ConversationalInterface:\n    def __init__(self):\n        self.context = {}\n        self.dialogue_history = []\n        \n    def process_interaction(self, user_input, modalities):\n        """Process multi-modal user input in context."""\n        # Update context\n        self.update_context(user_input, modalities)\n        \n        # Generate response\n        response = self.generate_response(\n            user_input,\n            self.context,\n            self.dialogue_history\n        )\n        \n        # Update history\n        self.dialogue_history.append((user_input, response))\n        \n        return response\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"5-exercises",children:"5. Exercises"}),"\n",(0,s.jsx)(n.h3,{id:"exercise-741-multi-modal-fusion",children:"Exercise 7.4.1: Multi-Modal Fusion"}),"\n",(0,s.jsx)(n.p,{children:"Combine voice command with visual detection."}),"\n",(0,s.jsx)(t,{id:"ex-7-4-1",title:"Speech + Vision Fusion",starterCode:'def fuse_speech_vision(voice_command, detected_objects):\n  """\n  Fuse voice command with visual object detection.\n  \n  Args:\n      voice_command: "Pick up the red cup"\n      detected_objects: [{"name": "cup", "color": "red", "position": [x, y]}]\n  \n  Returns:\n      Action with target object\n  """\n  # TODO: Extract object from voice, match to detected objects\n  pass\n\n# Test\ncommand = "Pick up the red cup"\nobjects = [\n  {"name": "cup", "color": "red", "position": [100, 200]},\n  {"name": "bottle", "color": "blue", "position": [300, 400]}\n]\n\naction = fuse_speech_vision(command, objects)\nprint(action)\n',hints:["Extract object name and attributes from voice command","Match extracted attributes to detected objects","Return action with matched object"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Key Takeaways"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Multi-modal interaction improves robustness and naturalness"}),"\n",(0,s.jsx)(n.li,{children:"Sensor fusion combines complementary information"}),"\n",(0,s.jsx)(n.li,{children:"Gesture recognition adds non-verbal communication"}),"\n",(0,s.jsx)(n.li,{children:"Conversational interfaces enable natural dialogue"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"What's Next"}),": ",(0,s.jsx)(n.a,{href:"/physical-ai-book/ur/docs/chapter-07/lesson-05-capstone",children:"Lesson 7.5: Capstone Project"})," - Build the complete Autonomous Humanoid system!"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Estimated completion time"}),": 60 minutes | ",(0,s.jsx)(n.strong,{children:"Prerequisites"}),": Lessons 7.1-7.3 | ",(0,s.jsx)(n.strong,{children:"Difficulty"}),": B2+ (Upper Advanced)"]})]})}function u(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>a});var i=t(6540);const s={},o=i.createContext(s);function r(e){const n=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),i.createElement(o.Provider,{value:n},e.children)}}}]);