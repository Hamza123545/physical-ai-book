if(void 0===__webpack_require__)var __webpack_require__={};(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[8877],{6245:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>r,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"chapter-06/lesson-05","title":"Lesson 6.5: Human-Robot Interaction","description":"Natural interaction design, gesture recognition, social cues, proxemics","source":"@site/i18n/ur/docusaurus-plugin-content-docs/current/chapter-06/lesson-05.md","sourceDirName":"chapter-06","slug":"/chapter-06/lesson-05","permalink":"/physical-ai-book/ur/docs/chapter-06/lesson-05","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"human-robot-interaction","permalink":"/physical-ai-book/ur/docs/tags/human-robot-interaction"},{"inline":true,"label":"gesture-recognition","permalink":"/physical-ai-book/ur/docs/tags/gesture-recognition"},{"inline":true,"label":"proxemics","permalink":"/physical-ai-book/ur/docs/tags/proxemics"},{"inline":true,"label":"social-robotics","permalink":"/physical-ai-book/ur/docs/tags/social-robotics"}],"version":"current","frontMatter":{"title":"Lesson 6.5: Human-Robot Interaction","description":"Natural interaction design, gesture recognition, social cues, proxemics","chapter":6,"lesson":5,"estimated_time":60,"cefr_level":"B2","blooms_level":"Apply","digcomp_level":6,"generated_by":"claude-sonnet-4-5-20250929","source_spec":"specs/002-physical-ai-textbook/spec.md","created":"2025-11-29","last_modified":"2025-11-29","git_author":"hswat","workflow":"/sp.implement","version":"1.0","prerequisites":["chapter-06-lesson-04","chapter-03-lesson-04"],"has_interactive_python":false,"interactive_python_count":0,"has_try_with_ai":true,"try_with_ai_count":2,"tags":["human-robot-interaction","gesture-recognition","proxemics","social-robotics"]},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 6.4: Compliant Control","permalink":"/physical-ai-book/ur/docs/chapter-06/lesson-04"},"next":{"title":"Chapter 7: Vision-Language-Action (VLA)","permalink":"/physical-ai-book/ur/docs/chapter-07/"}}');var s=t(4848),a=t(8453);const r={title:"Lesson 6.5: Human-Robot Interaction",description:"Natural interaction design, gesture recognition, social cues, proxemics",chapter:6,lesson:5,estimated_time:60,cefr_level:"B2",blooms_level:"Apply",digcomp_level:6,generated_by:"claude-sonnet-4-5-20250929",source_spec:"specs/002-physical-ai-textbook/spec.md",created:"2025-11-29",last_modified:"2025-11-29",git_author:"hswat",workflow:"/sp.implement",version:"1.0",prerequisites:["chapter-06-lesson-04","chapter-03-lesson-04"],has_interactive_python:!1,interactive_python_count:0,has_try_with_ai:!0,try_with_ai_count:2,tags:["human-robot-interaction","gesture-recognition","proxemics","social-robotics"]},o="Lesson 6.5: Human-Robot Interaction",l={},c=[{value:"Introduction",id:"introduction",level:2},{value:"1. Gesture Recognition",id:"1-gesture-recognition",level:2},{value:"2. Proxemics (Personal Space)",id:"2-proxemics-personal-space",level:2},{value:"3. Gaze and Attention",id:"3-gaze-and-attention",level:2},{value:"4. Expressive Motion",id:"4-expressive-motion",level:2},{value:"Interactive Exercises",id:"interactive-exercises",level:2},{value:"Exercise 6.5.1: Hand Gesture Classifier",id:"exercise-651-hand-gesture-classifier",level:3},{value:"Exercise 6.5.2: Proxemics-Based Approach Controller",id:"exercise-652-proxemics-based-approach-controller",level:3},{value:"Exercise 6.5.3: Gaze Control for Joint Attention",id:"exercise-653-gaze-control-for-joint-attention",level:3},{value:"Exercise 6.5.4: Expressive Gesture Generation",id:"exercise-654-expressive-gesture-generation",level:3},{value:"TryWithAI Exercises",id:"trywithai-exercises",level:2},{value:"TryWithAI 6.5.1: Multi-Gesture Interaction System",id:"trywithai-651-multi-gesture-interaction-system",level:3},{value:"TryWithAI 6.5.2: Adaptive Proxemics with Comfort Estimation",id:"trywithai-652-adaptive-proxemics-with-comfort-estimation",level:3},{value:"Summary",id:"summary",level:2}];function d(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"lesson-65-human-robot-interaction",children:"Lesson 6.5: Human-Robot Interaction"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Prerequisites"}),": ",(0,s.jsx)(n.a,{href:"/physical-ai-book/ur/docs/chapter-06/lesson-04",children:"Lesson 6.4: Manipulation and Grasping"}),", ",(0,s.jsx)(n.a,{href:"/physical-ai-book/ur/docs/chapter-03/",children:"Chapter 3: Sensors and Perception"})]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,s.jsx)(n.p,{children:"Humanoid robots must communicate and collaborate with humans naturally. Human-Robot Interaction (HRI) encompasses gesture recognition, social cues, proxemics (personal space), and multimodal communication. This lesson introduces computational models for interpreting human gestures, generating robot responses, and maintaining socially appropriate behaviors."}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Learning Objectives"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Recognize simple hand gestures from joint positions"}),"\n",(0,s.jsx)(n.li,{children:"Implement proxemics zones for personal space management"}),"\n",(0,s.jsx)(n.li,{children:"Design gaze control for social engagement"}),"\n",(0,s.jsx)(n.li,{children:"Generate expressive robot motions for communication"}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"1-gesture-recognition",children:"1. Gesture Recognition"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Gesture Classification"}),": Map hand/arm positions to semantic actions."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Approach"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Extract hand position relative to body (normalized coordinates)"}),"\n",(0,s.jsx)(n.li,{children:"Compute geometric features (height, distance from torso, arm angle)"}),"\n",(0,s.jsx)(n.li,{children:"Classify using rule-based logic or machine learning"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Common Gestures"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Wave"}),": Hand moving side-to-side above shoulder"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Point"}),": Arm extended, hand at target direction"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Stop"}),": Palm forward, arm extended"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Beckoning"}),": Hand moving toward body repeatedly"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"2-proxemics-personal-space",children:"2. Proxemics (Personal Space)"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Proxemics Zones"})," (Hall, 1966):"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Intimate"}),": 0-0.45m (close family/friends)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Personal"}),": 0.45-1.2m (casual interaction)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Social"}),": 1.2-3.6m (formal interaction)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Public"}),": >3.6m (public speaking)"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Robot Behavior"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Approach slowly when entering personal/intimate zones"}),"\n",(0,s.jsx)(n.li,{children:"Maintain social distance (1.2-1.5m) for initial interaction"}),"\n",(0,s.jsx)(n.li,{children:"Adjust speed and proximity based on user comfort signals"}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"3-gaze-and-attention",children:"3. Gaze and Attention"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Gaze Functions"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Joint Attention"}),": Look where human is looking (shared focus)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Turn-Taking"}),": Look at speaker during conversation, look away when thinking"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Engagement"}),": Maintain eye contact ~60% of time (too much is uncomfortable)"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Implementation"}),": Control head/eye orientation to track human face or objects."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"4-expressive-motion",children:"4. Expressive Motion"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Laban Effort Factors"})," (motion expressiveness):"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Weight"}),": Light (gentle) vs. Strong (forceful)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Time"}),": Sustained (slow) vs. Sudden (quick)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Space"}),": Direct (straight) vs. Indirect (curved)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Flow"}),": Free (flowing) vs. Bound (controlled)"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Example"}),': "Happy" gesture \u2192 Light, Sudden, Indirect, Free']}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"interactive-exercises",children:"Interactive Exercises"}),"\n",(0,s.jsx)(n.h3,{id:"exercise-651-hand-gesture-classifier",children:"Exercise 6.5.1: Hand Gesture Classifier"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"\"\"\"Classify hand gestures from 3D hand position.\"\"\"\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom typing import Tuple\n\ndef extract_gesture_features(\n    hand_pos: np.ndarray,\n    shoulder_pos: np.ndarray,\n    elbow_pos: np.ndarray\n) -> dict:\n    \"\"\"\n    Compute geometric features for gesture recognition.\n\n    Args:\n        hand_pos: Hand position [x, y, z] (m)\n        shoulder_pos: Shoulder position [x, y, z] (m)\n        elbow_pos: Elbow position [x, y, z] (m)\n\n    Returns:\n        features: Dictionary of geometric features\n    \"\"\"\n    # Relative positions\n    hand_rel = hand_pos - shoulder_pos\n    arm_vector = hand_pos - elbow_pos\n\n    # Features\n    features = {\n        'height': hand_rel[2],  # Height above shoulder\n        'lateral': abs(hand_rel[1]),  # Lateral distance from body\n        'forward': hand_rel[0],  # Forward distance\n        'arm_extension': np.linalg.norm(hand_pos - shoulder_pos),\n        'arm_angle_vertical': np.arctan2(arm_vector[2], np.linalg.norm(arm_vector[:2]))\n    }\n\n    return features\n\ndef classify_gesture(features: dict) -> str:\n    \"\"\"\n    Classify gesture from features (rule-based).\n\n    Args:\n        features: Gesture features\n\n    Returns:\n        gesture_name: Recognized gesture\n    \"\"\"\n    height = features['height']\n    lateral = features['lateral']\n    forward = features['forward']\n    extension = features['arm_extension']\n    angle = features['arm_angle_vertical']\n\n    # Rule-based classification\n    if height > 0.2 and lateral > 0.2:\n        return 'wave'\n    elif forward > 0.5 and angle < np.deg2rad(20):\n        return 'point'\n    elif forward > 0.4 and abs(angle) < np.deg2rad(30) and height < 0.1:\n        return 'stop'\n    elif forward < 0.1 and height < 0.0:\n        return 'beckon'\n    else:\n        return 'neutral'\n\n# Test gestures\nshoulder = np.array([0.0, 0.0, 0.0])\n\ngestures_data = {\n    'wave': {\n        'hand': np.array([0.1, 0.4, 0.3]),\n        'elbow': np.array([0.0, 0.2, 0.0])\n    },\n    'point': {\n        'hand': np.array([0.6, 0.1, 0.0]),\n        'elbow': np.array([0.3, 0.05, -0.05])\n    },\n    'stop': {\n        'hand': np.array([0.5, 0.0, 0.0]),\n        'elbow': np.array([0.2, 0.0, -0.1])\n    },\n    'beckon': {\n        'hand': np.array([0.0, 0.1, -0.2]),\n        'elbow': np.array([0.0, 0.1, -0.1])\n    },\n    'neutral': {\n        'hand': np.array([0.0, 0.0, -0.3]),\n        'elbow': np.array([0.0, 0.0, -0.15])\n    }\n}\n\n# Visualize and classify\nfig = plt.figure(figsize=(15, 3))\n\nfor i, (gesture_name, data) in enumerate(gestures_data.items()):\n    hand = data['hand']\n    elbow = data['elbow']\n\n    # Extract features\n    features = extract_gesture_features(hand, shoulder, elbow)\n    predicted = classify_gesture(features)\n\n    # Visualize\n    ax = fig.add_subplot(1, 5, i+1, projection='3d')\n\n    # Draw arm\n    points = np.array([shoulder, elbow, hand])\n    ax.plot(points[:, 0], points[:, 1], points[:, 2], 'b-o', linewidth=3, markersize=8)\n\n    # Highlight hand\n    ax.scatter(*hand, s=200, c='red', marker='*', edgecolors='black', linewidths=2)\n\n    # Labels\n    ax.set_xlabel('X (m)')\n    ax.set_ylabel('Y (m)')\n    ax.set_zlabel('Z (m)')\n    ax.set_title(f'{gesture_name.capitalize()}\\nPredicted: {predicted}')\n\n    # Equal aspect\n    ax.set_xlim(-0.2, 0.8)\n    ax.set_ylim(-0.3, 0.5)\n    ax.set_zlim(-0.4, 0.4)\n\nplt.tight_layout()\nplt.show()\n\n# Print features\nprint(\"\\nGesture Features:\")\nfor gesture_name, data in gestures_data.items():\n    features = extract_gesture_features(data['hand'], shoulder, data['elbow'])\n    predicted = classify_gesture(features)\n    match = \"\u2713\" if predicted == gesture_name else \"\u2717\"\n    print(f\"{gesture_name:8s}: height={features['height']:+.2f}m, \" +\n          f\"forward={features['forward']:+.2f}m \u2192 {predicted:8s} {match}\")\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Expected Output"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"3D visualization of arm poses for each gesture"}),"\n",(0,s.jsx)(n.li,{children:"Feature extraction (height, extension, angle)"}),"\n",(0,s.jsx)(n.li,{children:"Rule-based classification achieves 100% on test set"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"What You Learned"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Gestures defined by geometric relationships (hand relative to body)"}),"\n",(0,s.jsx)(n.li,{children:"Simple features (height, distance, angle) enable classification"}),"\n",(0,s.jsx)(n.li,{children:"Rule-based approach interpretable but limited to predefined gestures"}),"\n",(0,s.jsx)(n.li,{children:"Real systems use ML (e.g., SVM, neural networks) for robustness"}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"exercise-652-proxemics-based-approach-controller",children:"Exercise 6.5.2: Proxemics-Based Approach Controller"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"\"\"\"Control robot approach speed based on proxemics zones.\"\"\"\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom typing import Tuple\n\ndef proxemics_velocity(\n    distance: float,\n    comfort_distance: float = 1.2\n) -> float:\n    \"\"\"\n    Compute approach velocity based on proxemics.\n\n    Args:\n        distance: Current distance to human (m)\n        comfort_distance: Preferred social distance (m)\n\n    Returns:\n        velocity: Approach velocity (m/s)\n    \"\"\"\n    # Zone definitions\n    intimate = 0.45\n    personal = 1.2\n    social = 3.6\n\n    if distance > social:\n        # Public zone: fast approach\n        v = 0.5\n    elif distance > personal:\n        # Social zone: moderate approach\n        v = 0.3\n    elif distance > intimate:\n        # Personal zone: slow approach\n        v = 0.1\n    else:\n        # Intimate zone: stop or retreat\n        v = -0.2 if distance < comfort_distance else 0.0\n\n    # Smooth transition (optional)\n    target_distance = comfort_distance\n    error = distance - target_distance\n\n    # Simple proportional control\n    v = 0.5 * error\n    v = np.clip(v, -0.3, 0.5)  # Limit velocity\n\n    return v\n\ndef simulate_approach(\n    initial_distance: float = 5.0,\n    comfort_distance: float = 1.2,\n    duration: float = 15.0,\n    dt: float = 0.1\n) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Simulate robot approaching human with proxemics control.\n\n    Args:\n        initial_distance: Starting distance (m)\n        comfort_distance: Target social distance (m)\n        duration: Simulation time (s)\n        dt: Time step (s)\n\n    Returns:\n        times, distances\n    \"\"\"\n    times = np.arange(0, duration, dt)\n    distances = np.zeros(len(times))\n\n    distance = initial_distance\n\n    for i, t in enumerate(times):\n        distances[i] = distance\n\n        # Compute velocity\n        v = proxemics_velocity(distance, comfort_distance)\n\n        # Update distance (robot moves toward human)\n        distance -= v * dt\n\n        # Human might move (add noise)\n        if 5.0 < t < 7.0:\n            distance += 0.05 * dt  # Human steps back\n\n    return times, distances\n\n# Simulate approach\ntimes, distances = simulate_approach(\n    initial_distance=5.0,\n    comfort_distance=1.2\n)\n\n# Visualize\nfig, axes = plt.subplots(2, 1, figsize=(10, 8))\n\n# Distance over time\naxes[0].plot(times, distances, 'b-', linewidth=2, label='Robot-Human Distance')\n\n# Proxemics zones\naxes[0].axhspan(0, 0.45, alpha=0.2, color='red', label='Intimate')\naxes[0].axhspan(0.45, 1.2, alpha=0.2, color='orange', label='Personal')\naxes[0].axhspan(1.2, 3.6, alpha=0.2, color='yellow', label='Social')\naxes[0].axhspan(3.6, 6, alpha=0.2, color='green', label='Public')\n\naxes[0].axhline(1.2, color='black', linestyle='--', linewidth=2, label='Target')\naxes[0].set_ylabel('Distance (m)')\naxes[0].set_title('Proxemics-Based Approach Control')\naxes[0].legend(loc='upper right')\naxes[0].grid(True, alpha=0.3)\naxes[0].set_ylim(0, 6)\n\n# Velocity\nvelocities = np.array([proxemics_velocity(d, 1.2) for d in distances])\naxes[1].plot(times, velocities, 'g-', linewidth=2)\naxes[1].axhline(0, color='black', linestyle='-', alpha=0.3)\naxes[1].set_xlabel('Time (s)')\naxes[1].set_ylabel('Velocity (m/s)')\naxes[1].set_title('Approach Velocity (positive = toward human)')\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Analyze\nfinal_distance = distances[-1]\nsettling_idx = np.where(np.abs(distances - 1.2) < 0.1)[0]\nsettling_time = times[settling_idx[0]] if len(settling_idx) > 0 else np.inf\n\nprint(f\"Final distance: {final_distance:.2f} m (target: 1.2 m)\")\nprint(f\"Settling time: {settling_time:.1f} s\")\nprint(f\"Human retreat detected: {5.0 < settling_time}\")\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Expected Output"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Robot slows as it enters personal zone"}),"\n",(0,s.jsx)(n.li,{children:"Stops at social distance (1.2m)"}),"\n",(0,s.jsx)(n.li,{children:"Velocity decreases smoothly (no abrupt stops)"}),"\n",(0,s.jsx)(n.li,{children:"Small retreat when human steps back"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"What You Learned"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Proxemics zones define socially appropriate distances"}),"\n",(0,s.jsx)(n.li,{children:"Velocity should decrease as robot approaches human"}),"\n",(0,s.jsx)(n.li,{children:"Proportional control maintains comfortable spacing"}),"\n",(0,s.jsx)(n.li,{children:"Respecting personal space improves human acceptance"}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"exercise-653-gaze-control-for-joint-attention",children:"Exercise 6.5.3: Gaze Control for Joint Attention"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"\"\"\"Implement gaze control to track human attention.\"\"\"\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom typing import Tuple\n\ndef compute_gaze_target(\n    robot_head_pos: np.ndarray,\n    human_face_pos: np.ndarray,\n    object_pos: np.ndarray,\n    mode: str = 'face'\n) -> np.ndarray:\n    \"\"\"\n    Compute gaze target based on interaction mode.\n\n    Args:\n        robot_head_pos: Robot head position [x, y, z] (m)\n        human_face_pos: Human face position [x, y, z] (m)\n        object_pos: Object of interest position [x, y, z] (m)\n        mode: 'face' (look at human) or 'joint' (look at object)\n\n    Returns:\n        gaze_direction: Unit vector of gaze direction\n    \"\"\"\n    if mode == 'face':\n        target = human_face_pos\n    elif mode == 'joint':\n        target = object_pos\n    else:\n        target = human_face_pos\n\n    direction = target - robot_head_pos\n    gaze_direction = direction / (np.linalg.norm(direction) + 1e-6)\n\n    return gaze_direction\n\ndef head_orientation_from_gaze(\n    gaze_direction: np.ndarray\n) -> Tuple[float, float]:\n    \"\"\"\n    Compute head pan/tilt angles from gaze direction.\n\n    Args:\n        gaze_direction: Unit gaze vector [x, y, z]\n\n    Returns:\n        pan, tilt: Head angles (rad)\n    \"\"\"\n    # Pan (yaw): rotation around Z axis\n    pan = np.arctan2(gaze_direction[1], gaze_direction[0])\n\n    # Tilt (pitch): elevation angle\n    tilt = np.arcsin(gaze_direction[2])\n\n    return pan, tilt\n\n# Simulate gaze behavior during interaction\nduration = 10.0\ndt = 0.1\ntimes = np.arange(0, duration, dt)\n\n# Positions\nrobot_head = np.array([0.0, 0.0, 1.5])  # Robot head at 1.5m height\nhuman_face = np.array([1.5, 0.3, 1.6])  # Human 1.5m away, slightly to side\nobject_pos = np.array([1.0, -0.5, 1.0])  # Object on table\n\n# Gaze mode over time\n# 0-3s: Look at human (engagement)\n# 3-7s: Look at object (joint attention)\n# 7-10s: Look back at human (turn-taking)\n\npan_angles = np.zeros(len(times))\ntilt_angles = np.zeros(len(times))\nmodes = []\n\nfor i, t in enumerate(times):\n    if t < 3.0:\n        mode = 'face'\n    elif t < 7.0:\n        mode = 'joint'\n    else:\n        mode = 'face'\n\n    modes.append(mode)\n\n    gaze_dir = compute_gaze_target(robot_head, human_face, object_pos, mode)\n    pan, tilt = head_orientation_from_gaze(gaze_dir)\n\n    pan_angles[i] = pan\n    tilt_angles[i] = tilt\n\n# Visualize\nfig = plt.figure(figsize=(14, 10))\n\n# 3D scene\nax1 = fig.add_subplot(2, 2, 1, projection='3d')\n\nax1.scatter(*robot_head, s=300, c='blue', marker='o', label='Robot Head', edgecolors='black', linewidths=2)\nax1.scatter(*human_face, s=300, c='green', marker='s', label='Human Face', edgecolors='black', linewidths=2)\nax1.scatter(*object_pos, s=200, c='red', marker='^', label='Object', edgecolors='black', linewidths=2)\n\n# Gaze rays at key times\nfor t_sample, color in [(1.0, 'green'), (5.0, 'red'), (8.0, 'green')]:\n    idx = int(t_sample / dt)\n    if modes[idx] == 'face':\n        target = human_face\n    else:\n        target = object_pos\n\n    direction = (target - robot_head) * 0.8\n    ax1.quiver(*robot_head, *direction, color=color, arrow_length_ratio=0.1, linewidth=2, alpha=0.7)\n\nax1.set_xlabel('X (m)')\nax1.set_ylabel('Y (m)')\nax1.set_zlabel('Z (m)')\nax1.set_title('Gaze Scene (Top View)')\nax1.legend()\nax1.set_xlim(-0.5, 2)\nax1.set_ylim(-1, 1)\nax1.set_zlim(0.5, 2)\n\n# Pan angle\nax2 = fig.add_subplot(2, 2, 2)\nax2.plot(times, np.rad2deg(pan_angles), 'b-', linewidth=2)\nax2.fill_between(times, -90, 90, where=[m == 'face' for m in modes], alpha=0.2,\n                  color='green', label='Look at Human')\nax2.fill_between(times, -90, 90, where=[m == 'joint' for m in modes], alpha=0.2,\n                  color='red', label='Look at Object')\nax2.set_ylabel('Pan Angle (degrees)')\nax2.set_title('Head Pan (Yaw)')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\n# Tilt angle\nax3 = fig.add_subplot(2, 2, 3)\nax3.plot(times, np.rad2deg(tilt_angles), 'g-', linewidth=2)\nax3.fill_between(times, -45, 45, where=[m == 'face' for m in modes], alpha=0.2, color='green')\nax3.fill_between(times, -45, 45, where=[m == 'joint' for m in modes], alpha=0.2, color='red')\nax3.set_xlabel('Time (s)')\nax3.set_ylabel('Tilt Angle (degrees)')\nax3.set_title('Head Tilt (Pitch)')\nax3.grid(True, alpha=0.3)\n\n# Gaze mode\nax4 = fig.add_subplot(2, 2, 4)\nmode_values = [1 if m == 'face' else 0 for m in modes]\nax4.fill_between(times, 0, 1, where=[m == 1 for m in mode_values], alpha=0.5,\n                  color='green', label='Face', step='post')\nax4.fill_between(times, 0, 1, where=[m == 0 for m in mode_values], alpha=0.5,\n                  color='red', label='Object', step='post')\nax4.set_xlabel('Time (s)')\nax4.set_ylabel('Gaze Mode')\nax4.set_title('Gaze Target Over Time')\nax4.set_ylim(-0.1, 1.1)\nax4.legend()\nax4.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Print key angles\nprint(\"Head Orientations:\")\nfor t_sample in [1.0, 5.0, 8.0]:\n    idx = int(t_sample / dt)\n    print(f\"t={t_sample:.1f}s ({modes[idx]}): Pan={np.rad2deg(pan_angles[idx]):+.1f}\xb0, \" +\n          f\"Tilt={np.rad2deg(tilt_angles[idx]):+.1f}\xb0\")\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Expected Output"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"3D scene shows robot, human, and object positions"}),"\n",(0,s.jsx)(n.li,{children:"Head angles shift between human face and object"}),"\n",(0,s.jsx)(n.li,{children:"Smooth transitions during gaze shifts"}),"\n",(0,s.jsx)(n.li,{children:"Color-coded gaze modes (green=face, red=object)"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"What You Learned"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Gaze control directs robot attention to targets"}),"\n",(0,s.jsx)(n.li,{children:"Joint attention (looking at shared object) improves collaboration"}),"\n",(0,s.jsx)(n.li,{children:"Alternating gaze between human and object signals engagement"}),"\n",(0,s.jsx)(n.li,{children:"Pan/tilt angles computed from 3D gaze direction"}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"exercise-654-expressive-gesture-generation",children:"Exercise 6.5.4: Expressive Gesture Generation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"\"\"\"Generate expressive robot gestures using Laban effort factors.\"\"\"\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom typing import Tuple\n\ndef generate_expressive_trajectory(\n    start_pos: np.ndarray,\n    end_pos: np.ndarray,\n    duration: float,\n    weight: str = 'light',\n    time_quality: str = 'sustained',\n    space: str = 'direct',\n    dt: float = 0.01\n) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Generate trajectory with Laban effort qualities.\n\n    Args:\n        start_pos: Starting position [x, y] (m)\n        end_pos: Ending position [x, y] (m)\n        duration: Motion duration (s)\n        weight: 'light' or 'strong'\n        time_quality: 'sustained' or 'sudden'\n        space: 'direct' or 'indirect'\n        dt: Time step (s)\n\n    Returns:\n        times, positions\n    \"\"\"\n    times = np.arange(0, duration, dt)\n    positions = np.zeros((len(times), 2))\n\n    for i, t in enumerate(times):\n        # Normalized time [0, 1]\n        s = t / duration\n\n        # Time quality (velocity profile)\n        if time_quality == 'sustained':\n            # Slow start and end (ease-in-out)\n            s_smooth = 3*s**2 - 2*s**3\n        else:  # sudden\n            # Quick motion (linear or accelerating)\n            s_smooth = s**2\n\n        # Space quality (path curvature)\n        if space == 'direct':\n            # Straight line\n            pos = start_pos + s_smooth * (end_pos - start_pos)\n        else:  # indirect\n            # Curved path (add sinusoidal deviation)\n            midpoint = (start_pos + end_pos) / 2\n            perpendicular = np.array([-(end_pos[1] - start_pos[1]), end_pos[0] - start_pos[0]])\n            perpendicular = perpendicular / (np.linalg.norm(perpendicular) + 1e-6)\n\n            curvature = 0.2 * np.sin(s * np.pi)  # Arc\n            pos = start_pos + s_smooth * (end_pos - start_pos) + curvature * perpendicular\n\n        positions[i] = pos\n\n    return times, positions\n\n# Generate gestures with different expressive qualities\nstart = np.array([0.0, 0.0])\nend = np.array([0.5, 0.3])\nduration = 2.0\n\ngestures = {\n    'Happy (Light, Sudden, Indirect)': ('light', 'sudden', 'indirect'),\n    'Careful (Light, Sustained, Direct)': ('light', 'sustained', 'direct'),\n    'Forceful (Strong, Sudden, Direct)': ('strong', 'sudden', 'direct'),\n    'Gentle (Light, Sustained, Indirect)': ('light', 'sustained', 'indirect')\n}\n\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\naxes = axes.flat\n\nfor ax, (name, (weight, time_q, space)) in zip(axes, gestures.items()):\n    # Adjust duration based on time quality\n    dur = duration if time_q == 'sustained' else duration * 0.5\n\n    times, positions = generate_expressive_trajectory(\n        start, end, dur, weight, time_q, space\n    )\n\n    # Plot trajectory\n    ax.plot(positions[:, 0], positions[:, 1], 'b-', linewidth=2, label='Path')\n    ax.scatter(*start, s=200, c='green', marker='o', edgecolors='black',\n               linewidths=2, label='Start', zorder=5)\n    ax.scatter(*end, s=200, c='red', marker='X', edgecolors='black',\n               linewidths=2, label='End', zorder=5)\n\n    # Velocity arrows (show time quality)\n    for i in range(0, len(times), max(1, len(times)//10)):\n        if i < len(times) - 1:\n            vel = (positions[i+1] - positions[i]) / (times[1] - times[0])\n            ax.arrow(positions[i, 0], positions[i, 1], vel[0]*0.02, vel[1]*0.02,\n                    head_width=0.02, head_length=0.01, fc='orange', ec='orange', alpha=0.6)\n\n    ax.set_xlabel('X (m)')\n    ax.set_ylabel('Y (m)')\n    ax.set_title(name)\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    ax.set_xlim(-0.1, 0.6)\n    ax.set_ylim(-0.2, 0.5)\n    ax.set_aspect('equal')\n\nplt.tight_layout()\nplt.show()\n\n# Analyze motion characteristics\nfor name, (weight, time_q, space) in gestures.items():\n    dur = duration if time_q == 'sustained' else duration * 0.5\n    times, positions = generate_expressive_trajectory(start, end, dur, weight, time_q, space)\n\n    velocities = np.diff(positions, axis=0) / (times[1] - times[0])\n    avg_speed = np.mean(np.linalg.norm(velocities, axis=1))\n    max_speed = np.max(np.linalg.norm(velocities, axis=1))\n    path_length = np.sum(np.linalg.norm(np.diff(positions, axis=0), axis=1))\n\n    print(f\"{name}:\")\n    print(f\"  Duration: {dur:.2f}s, Avg speed: {avg_speed:.3f} m/s, \" +\n          f\"Max speed: {max_speed:.3f} m/s, Path length: {path_length:.3f}m\")\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Expected Output"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Direct paths are straight; indirect paths curve"}),"\n",(0,s.jsx)(n.li,{children:"Sudden motions are faster (shorter duration)"}),"\n",(0,s.jsx)(n.li,{children:"Sustained motions have smooth velocity profiles"}),"\n",(0,s.jsx)(n.li,{children:"Curved paths indicate playfulness/gentleness"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"What You Learned"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Laban effort factors parameterize motion expressiveness"}),"\n",(0,s.jsx)(n.li,{children:"Time quality affects velocity profile (smooth vs. sharp)"}),"\n",(0,s.jsx)(n.li,{children:"Space quality affects path shape (straight vs. curved)"}),"\n",(0,s.jsx)(n.li,{children:"Expressive motion enhances robot communication"}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"trywithai-exercises",children:"TryWithAI Exercises"}),"\n",(0,s.jsx)(n.h3,{id:"trywithai-651-multi-gesture-interaction-system",children:"TryWithAI 6.5.1: Multi-Gesture Interaction System"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Prompt"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Build a real-time gesture-based robot control system. The system should:\n\n1. Recognize 5 gestures: wave (greeting), point (target selection), stop (halt), beckoning (come closer), thumbs-up (confirmation)\n2. Generate appropriate robot responses for each gesture (e.g., wave back, approach, stop motion, acknowledge)\n3. Include a state machine to manage interaction flow (idle \u2192 engaged \u2192 task \u2192 idle)\n4. Visualize human skeleton, detected gesture, and robot response in each frame\n\nUse synthetic gesture data (sequence of hand/arm positions). Show how the robot transitions between states based on gesture sequences.\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Expected Skills"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Multi-class gesture recognition"}),"\n",(0,s.jsx)(n.li,{children:"State machine design for interaction"}),"\n",(0,s.jsx)(n.li,{children:"Response generation (inverse action mapping)"}),"\n",(0,s.jsx)(n.li,{children:"Temporal gesture sequence handling"}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"trywithai-652-adaptive-proxemics-with-comfort-estimation",children:"TryWithAI 6.5.2: Adaptive Proxemics with Comfort Estimation"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Prompt"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Design a proxemics controller that learns individual user comfort preferences. The system should:\n\n1. Monitor user behavior cues (stepping back, crossing arms, gaze aversion)\n2. Estimate user comfort level (0-1 scale) from cues\n3. Adapt preferred distance based on estimated comfort (increase distance if discomfort detected)\n4. Store per-user preferences and recall for repeat interactions\n\nSimulate interactions with 3 user types: comfortable (allows 0.8m), neutral (1.2m), uncomfortable (1.8m). Show how the robot adapts its approach distance over multiple encounters.\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Expected Skills"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Behavioral cue detection"}),"\n",(0,s.jsx)(n.li,{children:"Online learning/adaptation"}),"\n",(0,s.jsx)(n.li,{children:"User modeling and personalization"}),"\n",(0,s.jsx)(n.li,{children:"Memory and recognition across sessions"}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(n.p,{children:"This lesson introduced human-robot interaction fundamentals:"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Key Concepts"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Gesture recognition maps body poses to semantic actions"}),"\n",(0,s.jsx)(n.li,{children:"Proxemics zones define socially appropriate distances"}),"\n",(0,s.jsx)(n.li,{children:"Gaze control enables joint attention and engagement"}),"\n",(0,s.jsx)(n.li,{children:"Expressive motion communicates robot intent and emotion"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Practical Skills"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Classifying hand gestures from geometric features"}),"\n",(0,s.jsx)(n.li,{children:"Implementing proxemics-based approach control"}),"\n",(0,s.jsx)(n.li,{children:"Controlling robot gaze for joint attention"}),"\n",(0,s.jsx)(n.li,{children:"Generating expressive trajectories with Laban qualities"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Next Chapter"}),": ",(0,s.jsx)(n.a,{href:"/physical-ai-book/ur/docs/chapter-07/",children:"Chapter 7: Mobile Manipulation"})," combines mobility and manipulation skills."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Further Reading"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Hall, E.T. (1966). ",(0,s.jsx)(n.em,{children:"The Hidden Dimension"}),". Anchor Books."]}),"\n",(0,s.jsxs)(n.li,{children:['Breazeal, C. (2003). "Toward Sociable Robots." ',(0,s.jsx)(n.em,{children:"Robotics and Autonomous Systems"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:["Laban, R., & Ullmann, L. (1971). ",(0,s.jsx)(n.em,{children:"The Mastery of Movement"}),". Princeton Book Company."]}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>o});var i=t(6540);const s={},a=i.createContext(s);function r(e){const n=i.useContext(a);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),i.createElement(a.Provider,{value:n},e.children)}}}]);