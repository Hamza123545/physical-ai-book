if(void 0===__webpack_require__)var __webpack_require__={};(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[4879],{8453:(e,n,s)=>{s.d(n,{R:()=>a,x:()=>l});var i=s(6540);const r={},t=i.createContext(r);function a(e){const n=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),i.createElement(t.Provider,{value:n},e.children)}},9773:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>h,frontMatter:()=>a,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"chapter-04/index","title":"Chapter 4: Reinforcement Learning for Robotics","description":"Learn how robots learn from trial and error using RL algorithms","source":"@site/docs/chapter-04/index.md","sourceDirName":"chapter-04","slug":"/chapter-04/","permalink":"/physical-ai-book/docs/chapter-04/","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"title":"Chapter 4: Reinforcement Learning for Robotics","description":"Learn how robots learn from trial and error using RL algorithms","sidebar_position":4,"chapter":4,"lessons":8,"estimated_time":500,"cefr_level":"B2","blooms_level":"Apply","digcomp_level":6},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 3.5: Unity for High-Fidelity Rendering","permalink":"/physical-ai-book/docs/chapter-03/lesson-05-unity-rendering"},"next":{"title":"Lesson 4.1: RL Basics and Markov Decision Processes","permalink":"/physical-ai-book/docs/chapter-04/lesson-01-rl-basics"}}');var r=s(4848),t=s(8453);const a={title:"Chapter 4: Reinforcement Learning for Robotics",description:"Learn how robots learn from trial and error using RL algorithms",sidebar_position:4,chapter:4,lessons:8,estimated_time:500,cefr_level:"B2",blooms_level:"Apply",digcomp_level:6},l="Chapter 4: Reinforcement Learning for Robotics",o={},c=[{value:"Overview",id:"overview",level:2},{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Chapter Structure",id:"chapter-structure",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Success Criteria",id:"success-criteria",level:2},{value:"\ud83d\udcdd Content Creation Workflow",id:"-content-creation-workflow",level:2},{value:"Lessons",id:"lessons",level:2},{value:"Lesson 4.1: RL Basics: MDP, Bellman Equation",id:"lesson-41-rl-basics-mdp-bellman-equation",level:3},{value:"Lesson 4.2: Q-Learning: Tabular RL",id:"lesson-42-q-learning-tabular-rl",level:3},{value:"Lesson 4.3: Deep Q-Networks: Neural RL",id:"lesson-43-deep-q-networks-neural-rl",level:3},{value:"Lesson 4.4: Policy Gradients: REINFORCE",id:"lesson-44-policy-gradients-reinforce",level:3},{value:"Lesson 4.5: Multi-Agent RL",id:"lesson-45-multi-agent-rl",level:3},{value:"Lesson 4.6: NVIDIA Isaac Sim - Photorealistic Simulation &amp; Synthetic Data",id:"lesson-46-nvidia-isaac-sim---photorealistic-simulation--synthetic-data",level:3},{value:"Lesson 4.7: Isaac ROS - Hardware-Accelerated VSLAM &amp; Navigation",id:"lesson-47-isaac-ros---hardware-accelerated-vslam--navigation",level:3},{value:"Lesson 4.8: Sim-to-Real Transfer Techniques",id:"lesson-48-sim-to-real-transfer-techniques",level:3},{value:"Chapter Assessment",id:"chapter-assessment",level:2},{value:"Chapter 4 Quiz",id:"chapter-4-quiz",level:3}];function d(e){const n={a:"a",br:"br",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"chapter-4-reinforcement-learning-for-robotics",children:"Chapter 4: Reinforcement Learning for Robotics"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Part of"}),": Module 3 - The AI-Robot Brain (NVIDIA Isaac\u2122) | Weeks 8-10",(0,r.jsx)(n.br,{}),"\n",(0,r.jsx)(n.strong,{children:"Note"}),": This chapter covers RL fundamentals and NVIDIA Isaac platform integration. Isaac Sim, Isaac ROS, VSLAM, and sim-to-real transfer are integrated throughout."]}),"\n",(0,r.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Reinforcement Learning (RL)"})," enables robots to learn optimal behaviors through trial and error. Instead of programming every action, robots discover strategies by receiving rewards for good actions and penalties for bad ones."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Applications"}),": Walking gaits, manipulation, autonomous navigation, game playing."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,r.jsx)(n.p,{children:"By the end of this chapter, you will:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Understand RL fundamentals"}),": States, actions, rewards, policies"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Implement Q-learning"}),": Tabular RL for discrete environments"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Build Deep Q-Networks (DQN)"}),": Scale RL to high-dimensional states"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Apply policy gradients"}),": Direct policy optimization"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Explore multi-agent RL"}),": Coordinating multiple robots"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Master NVIDIA Isaac Sim"}),": Photorealistic simulation and synthetic data generation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Use Isaac ROS"}),": Hardware-accelerated VSLAM (Visual SLAM) and navigation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Implement sim-to-real transfer"}),": Deploy trained models to physical robots"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"chapter-structure",children:"Chapter Structure"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Lesson"}),(0,r.jsx)(n.th,{children:"Topic"}),(0,r.jsx)(n.th,{children:"Time"}),(0,r.jsx)(n.th,{children:"Exercises"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"4.1"}),(0,r.jsx)(n.td,{children:"RL Basics: MDP, Bellman Equation"}),(0,r.jsx)(n.td,{children:"60 min"}),(0,r.jsx)(n.td,{children:"3 + 1 AI"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"4.2"}),(0,r.jsx)(n.td,{children:"Q-Learning: Tabular RL"}),(0,r.jsx)(n.td,{children:"60 min"}),(0,r.jsx)(n.td,{children:"3 + 2 AI"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"4.3"}),(0,r.jsx)(n.td,{children:"Deep Q-Networks: Neural RL"}),(0,r.jsx)(n.td,{children:"70 min"}),(0,r.jsx)(n.td,{children:"4 + 2 AI"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"4.4"}),(0,r.jsx)(n.td,{children:"Policy Gradients: REINFORCE"}),(0,r.jsx)(n.td,{children:"60 min"}),(0,r.jsx)(n.td,{children:"3 + 1 AI"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"4.5"}),(0,r.jsx)(n.td,{children:"Multi-Agent RL"}),(0,r.jsx)(n.td,{children:"60 min"}),(0,r.jsx)(n.td,{children:"3 + 2 AI"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"4.6"}),(0,r.jsx)(n.td,{children:"NVIDIA Isaac Sim: Photorealistic Simulation"}),(0,r.jsx)(n.td,{children:"70 min"}),(0,r.jsx)(n.td,{children:"4 + 2 AI"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"4.7"}),(0,r.jsx)(n.td,{children:"Isaac ROS: VSLAM and Navigation"}),(0,r.jsx)(n.td,{children:"60 min"}),(0,r.jsx)(n.td,{children:"3 + 1 AI"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"4.8"}),(0,r.jsx)(n.td,{children:"Sim-to-Real Transfer"}),(0,r.jsx)(n.td,{children:"60 min"}),(0,r.jsx)(n.td,{children:"3 + 2 AI"})]})]})]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Total"}),": 500 minutes (~8.3 hours)"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Note"}),": Lessons 4.6-4.8 focus specifically on NVIDIA Isaac platform integration, covering photorealistic simulation, hardware-accelerated perception, and deploying trained models to physical robots."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Chapter 1: Python and NumPy"}),"\n",(0,r.jsx)(n.li,{children:"Chapter 2: ROS 2 fundamentals (nodes, topics, services)"}),"\n",(0,r.jsx)(n.li,{children:"Chapter 3: Gazebo simulation basics"}),"\n",(0,r.jsx)(n.li,{children:"Basic understanding of optimization (gradient descent)"}),"\n",(0,r.jsx)(n.li,{children:"Comfort with probability and expected values"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Hardware Note"}),": NVIDIA Isaac Sim requires RTX-capable GPU (RTX 4070 Ti or higher recommended). For cloud-based alternatives, see course hardware requirements."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"success-criteria",children:"Success Criteria"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Train Q-learning agent to solve GridWorld with 90%+ success"}),"\n",(0,r.jsx)(n.li,{children:"Implement DQN for CartPole environment"}),"\n",(0,r.jsx)(n.li,{children:"Understand exploration vs. exploitation trade-offs"}),"\n",(0,r.jsx)(n.li,{children:"Compare value-based (Q-learning) vs. policy-based (REINFORCE) methods"}),"\n",(0,r.jsx)(n.li,{children:"Set up NVIDIA Isaac Sim environment with humanoid robot"}),"\n",(0,r.jsx)(n.li,{children:"Generate synthetic training data using Isaac Sim"}),"\n",(0,r.jsx)(n.li,{children:"Implement VSLAM pipeline using Isaac ROS"}),"\n",(0,r.jsx)(n.li,{children:"Successfully transfer trained RL policy from simulation to real robot (sim-to-real)"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"-content-creation-workflow",children:"\ud83d\udcdd Content Creation Workflow"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Workflow Note"}),": This chapter was created during the initial textbook development phase (November 2025) using direct content creation. Starting with Chapter 5, all content is created using the ",(0,r.jsx)(n.code,{children:"physical-ai-content-writer"})," subagent workflow, which enforces Constitution v6.0.0 compliance through:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"4-Layer Teaching Method"}),": Foundation \u2192 Application \u2192 Integration \u2192 Innovation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"AI Three Roles Framework"}),": Teacher, Copilot, Evaluator roles in exercises"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"CEFR Cognitive Load Management"}),": B1-B2 proficiency-appropriate content"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"SpecKit Plus Workflow"}),": Spec \u2192 Plan \u2192 Tasks \u2192 Implement with validation gates"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"This chapter has been audited for constitution compliance and contains all required pedagogical elements."}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"lessons",children:"Lessons"}),"\n",(0,r.jsx)(n.h3,{id:"lesson-41-rl-basics-mdp-bellman-equation",children:(0,r.jsx)(n.a,{href:"/physical-ai-book/docs/chapter-04/lesson-01-rl-basics",children:"Lesson 4.1: RL Basics: MDP, Bellman Equation"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Duration"}),": 60 minutes | ",(0,r.jsx)(n.strong,{children:"Difficulty"}),": B2 (Advanced)"]}),"\n",(0,r.jsx)(n.p,{children:"Learn the fundamentals of reinforcement learning: Markov Decision Processes (MDPs), value functions, and the Bellman equation."}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Key Concepts"}),": MDP, states, actions, rewards, value functions, Bellman equation"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Exercises"}),": 3 interactive Python exercises + 1 Try With AI"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"lesson-42-q-learning-tabular-rl",children:(0,r.jsx)(n.a,{href:"/physical-ai-book/docs/chapter-04/lesson-02-q-learning",children:"Lesson 4.2: Q-Learning: Tabular RL"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Duration"}),": 60 minutes | ",(0,r.jsx)(n.strong,{children:"Difficulty"}),": B2 (Advanced)"]}),"\n",(0,r.jsx)(n.p,{children:"Implement Q-learning, a tabular reinforcement learning algorithm for discrete environments."}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Key Concepts"}),": Q-table, Q-learning algorithm, epsilon-greedy exploration, Bellman update"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Exercises"}),": 3 interactive Python exercises + 2 Try With AI"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"lesson-43-deep-q-networks-neural-rl",children:(0,r.jsx)(n.a,{href:"/physical-ai-book/docs/chapter-04/lesson-03-deep-q-networks",children:"Lesson 4.3: Deep Q-Networks: Neural RL"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Duration"}),": 70 minutes | ",(0,r.jsx)(n.strong,{children:"Difficulty"}),": B2 (Advanced)"]}),"\n",(0,r.jsx)(n.p,{children:"Scale reinforcement learning to high-dimensional states using deep neural networks."}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Key Concepts"}),": DQN, experience replay, target networks, neural network approximation"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Exercises"}),": 4 interactive Python exercises + 2 Try With AI"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"lesson-44-policy-gradients-reinforce",children:(0,r.jsx)(n.a,{href:"/physical-ai-book/docs/chapter-04/lesson-04-policy-gradients",children:"Lesson 4.4: Policy Gradients: REINFORCE"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Duration"}),": 60 minutes | ",(0,r.jsx)(n.strong,{children:"Difficulty"}),": B2 (Advanced)"]}),"\n",(0,r.jsx)(n.p,{children:"Learn policy-based reinforcement learning methods that directly optimize policies."}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Key Concepts"}),": Policy gradients, REINFORCE algorithm, advantage estimation, policy networks"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Exercises"}),": 3 interactive Python exercises + 1 Try With AI"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"lesson-45-multi-agent-rl",children:(0,r.jsx)(n.a,{href:"/physical-ai-book/docs/chapter-04/lesson-05-multi-agent-rl",children:"Lesson 4.5: Multi-Agent RL"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Duration"}),": 60 minutes | ",(0,r.jsx)(n.strong,{children:"Difficulty"}),": B2 (Advanced)"]}),"\n",(0,r.jsx)(n.p,{children:"Coordinate multiple robots using multi-agent reinforcement learning."}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Key Concepts"}),": Multi-agent systems, independent Q-learning, cooperation, non-stationarity"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Exercises"}),": 3 interactive Python exercises + 2 Try With AI"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"lesson-46-nvidia-isaac-sim---photorealistic-simulation--synthetic-data",children:(0,r.jsx)(n.a,{href:"/physical-ai-book/docs/chapter-04/lesson-06-isaac-sim",children:"Lesson 4.6: NVIDIA Isaac Sim - Photorealistic Simulation & Synthetic Data"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Duration"}),": 70 minutes | ",(0,r.jsx)(n.strong,{children:"Difficulty"}),": B2 (Advanced)"]}),"\n",(0,r.jsx)(n.p,{children:"Master NVIDIA Isaac Sim for photorealistic robot simulation and synthetic data generation."}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Key Concepts"}),": Isaac Sim, photorealistic rendering, Replicator, domain randomization, synthetic data generation"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Exercises"}),": 4 interactive Python exercises + 2 Try With AI"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"lesson-47-isaac-ros---hardware-accelerated-vslam--navigation",children:(0,r.jsx)(n.a,{href:"/physical-ai-book/docs/chapter-04/lesson-07-isaac-ros",children:"Lesson 4.7: Isaac ROS - Hardware-Accelerated VSLAM & Navigation"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Duration"}),": 60 minutes | ",(0,r.jsx)(n.strong,{children:"Difficulty"}),": B2 (Advanced)"]}),"\n",(0,r.jsx)(n.p,{children:"Use Isaac ROS for hardware-accelerated visual SLAM and navigation."}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Key Concepts"}),": Isaac ROS, VSLAM, GPU acceleration, ROS 2 integration, navigation stack"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Exercises"}),": 3 interactive Python exercises + 1 Try With AI"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"lesson-48-sim-to-real-transfer-techniques",children:(0,r.jsx)(n.a,{href:"/physical-ai-book/docs/chapter-04/lesson-08-sim-to-real",children:"Lesson 4.8: Sim-to-Real Transfer Techniques"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Duration"}),": 60 minutes | ",(0,r.jsx)(n.strong,{children:"Difficulty"}),": B2 (Advanced)"]}),"\n",(0,r.jsx)(n.p,{children:"Master techniques for transferring models trained in simulation to physical robots."}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Key Concepts"}),": Sim-to-real gap, domain randomization, domain adaptation, fine-tuning, deployment strategies"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Exercises"}),": 3 interactive Python exercises + 2 Try With AI"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"chapter-assessment",children:"Chapter Assessment"}),"\n",(0,r.jsx)(n.h3,{id:"chapter-4-quiz",children:(0,r.jsx)(n.a,{href:"/physical-ai-book/docs/chapter-04/quiz",children:"Chapter 4 Quiz"})}),"\n",(0,r.jsx)(n.p,{children:"Test your understanding with a comprehensive quiz covering all RL and NVIDIA Isaac concepts."}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Ready?"})," Start with ",(0,r.jsx)(n.a,{href:"/physical-ai-book/docs/chapter-04/lesson-01-rl-basics",children:"Lesson 4.1: RL Basics"}),"!"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}}}]);