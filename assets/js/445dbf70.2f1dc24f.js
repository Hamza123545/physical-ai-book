if(void 0===__webpack_require__)var __webpack_require__={};(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[209],{8431:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>t,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"chapter-03/lesson-04-sensor-simulation","title":"Lesson 3.4: Sensor Simulation in Gazebo","description":"Simulate realistic sensor data: LiDAR, cameras, IMUs for perception algorithms","source":"@site/docs/chapter-03/lesson-04-sensor-simulation.md","sourceDirName":"chapter-03","slug":"/chapter-03/lesson-04-sensor-simulation","permalink":"/physical-ai-book/docs/chapter-03/lesson-04-sensor-simulation","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"gazebo","permalink":"/physical-ai-book/docs/tags/gazebo"},{"inline":true,"label":"sensors","permalink":"/physical-ai-book/docs/tags/sensors"},{"inline":true,"label":"lidar","permalink":"/physical-ai-book/docs/tags/lidar"},{"inline":true,"label":"cameras","permalink":"/physical-ai-book/docs/tags/cameras"},{"inline":true,"label":"imu","permalink":"/physical-ai-book/docs/tags/imu"},{"inline":true,"label":"simulation","permalink":"/physical-ai-book/docs/tags/simulation"}],"version":"current","frontMatter":{"title":"Lesson 3.4: Sensor Simulation in Gazebo","description":"Simulate realistic sensor data: LiDAR, cameras, IMUs for perception algorithms","chapter":3,"lesson":4,"estimated_time":70,"cefr_level":"B2","blooms_level":"Apply","digcomp_level":5,"generated_by":"claude-sonnet-4-5-20250929","source_spec":"specs/002-physical-ai-textbook/spec.md","created":"2025-11-29","last_modified":"2025-11-29","git_author":"hswat","workflow":"/sp.implement","version":"1.0","prerequisites":["chapter-03-lesson-03"],"has_interactive_python":true,"interactive_python_count":1,"has_try_with_ai":false,"try_with_ai_count":0,"tags":["gazebo","sensors","lidar","cameras","imu","simulation"]},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 3.3: Physics Simulation in Gazebo","permalink":"/physical-ai-book/docs/chapter-03/lesson-03-physics-simulation"},"next":{"title":"Lesson 3.5: Unity for High-Fidelity Rendering","permalink":"/physical-ai-book/docs/chapter-03/lesson-05-unity-rendering"}}');var a=s(4848),r=s(8453);const t={title:"Lesson 3.4: Sensor Simulation in Gazebo",description:"Simulate realistic sensor data: LiDAR, cameras, IMUs for perception algorithms",chapter:3,lesson:4,estimated_time:70,cefr_level:"B2",blooms_level:"Apply",digcomp_level:5,generated_by:"claude-sonnet-4-5-20250929",source_spec:"specs/002-physical-ai-textbook/spec.md",created:"2025-11-29",last_modified:"2025-11-29",git_author:"hswat",workflow:"/sp.implement",version:"1.0",prerequisites:["chapter-03-lesson-03"],has_interactive_python:!0,interactive_python_count:1,has_try_with_ai:!1,try_with_ai_count:0,tags:["gazebo","sensors","lidar","cameras","imu","simulation"]},o="Lesson 3.4: Sensor Simulation in Gazebo",l={},c=[{value:"\ud83c\udfaf Learning Objectives",id:"-learning-objectives",level:2},{value:"Introduction",id:"introduction",level:2},{value:"1. LiDAR Simulation",id:"1-lidar-simulation",level:2},{value:"LiDAR Plugin",id:"lidar-plugin",level:3},{value:"2. Depth Camera (RGB-D)",id:"2-depth-camera-rgb-d",level:2},{value:"Camera Plugin",id:"camera-plugin",level:3},{value:"3. IMU Sensor",id:"3-imu-sensor",level:2},{value:"IMU Plugin",id:"imu-plugin",level:3},{value:"4. Sensor Noise",id:"4-sensor-noise",level:2},{value:"5. Exercises",id:"5-exercises",level:2},{value:"Exercise 3.4.1: Process LiDAR Data",id:"exercise-341-process-lidar-data",level:3},{value:"Summary",id:"summary",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components},{InteractivePython:s}=n;return s||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("InteractivePython",!0),(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"lesson-34-sensor-simulation-in-gazebo",children:"Lesson 3.4: Sensor Simulation in Gazebo"})}),"\n",(0,a.jsx)(n.h2,{id:"-learning-objectives",children:"\ud83c\udfaf Learning Objectives"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Simulate LiDAR sensors for 3D scanning"}),"\n",(0,a.jsx)(n.li,{children:"Generate depth camera (RGB-D) data"}),"\n",(0,a.jsx)(n.li,{children:"Model IMU sensors for orientation and acceleration"}),"\n",(0,a.jsx)(n.li,{children:"Add sensor noise for realistic simulation"}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Time"}),": 70 minutes"]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,a.jsx)(n.p,{children:"Gazebo can simulate realistic sensor data, enabling you to develop and test perception algorithms without real hardware. This is crucial for training computer vision and SLAM systems."}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Key Sensors"}),":"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"LiDAR"}),": 3D point clouds for mapping"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Depth Cameras"}),": RGB-D data for object detection"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"IMUs"}),": Orientation and acceleration"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Cameras"}),": Visual perception"]}),"\n"]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"1-lidar-simulation",children:"1. LiDAR Simulation"}),"\n",(0,a.jsx)(n.h3,{id:"lidar-plugin",children:"LiDAR Plugin"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:'<sensor name="lidar" type="ray">\n  <pose>0 0 0.2 0 0 0</pose>\n  <visualize>true</visualize>\n  <update_rate>40</update_rate>\n  <ray>\n    <scan>\n      <horizontal>\n        <samples>720</samples>\n        <resolution>1</resolution>\n        <min_angle>-3.14159</min_angle>\n        <max_angle>3.14159</max_angle>\n      </horizontal>\n      <vertical>\n        <samples>1</samples>\n        <min_angle>0</min_angle>\n        <max_angle>0</max_angle>\n      </vertical>\n    </scan>\n    <range>\n      <min>0.1</min>\n      <max>30.0</max>\n      <resolution>0.01</resolution>\n    </range>\n  </ray>\n  <plugin name="gazebo_ros_laser_controller" filename="libgazebo_ros_laser.so">\n    <topicName>/scan</topicName>\n    <frameName>laser_frame</frameName>\n  </plugin>\n</sensor>\n'})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"2-depth-camera-rgb-d",children:"2. Depth Camera (RGB-D)"}),"\n",(0,a.jsx)(n.h3,{id:"camera-plugin",children:"Camera Plugin"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:'<sensor name="depth_camera" type="depth">\n  <pose>0 0 0.1 0 0 0</pose>\n  <visualize>true</visualize>\n  <update_rate>30</update_rate>\n  <camera name="camera1">\n    <horizontal_fov>1.047</horizontal_fov>\n    <image>\n      <width>640</width>\n      <height>480</height>\n      <format>R8G8B8</format>\n    </image>\n    <clip>\n      <near>0.05</near>\n      <far>3.0</far>\n    </clip>\n  </camera>\n  <plugin name="gazebo_ros_camera" filename="libgazebo_ros_camera.so">\n    <ros>\n      <namespace>/camera</namespace>\n    </ros>\n    <camera_name>camera1</camera_name>\n    <frame_name>camera_frame</frame_name>\n    <hack_baseline>0.07</hack_baseline>\n    <min_depth>0.05</min_depth>\n    <max_depth>3.0</max_depth>\n  </plugin>\n</sensor>\n'})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"3-imu-sensor",children:"3. IMU Sensor"}),"\n",(0,a.jsx)(n.h3,{id:"imu-plugin",children:"IMU Plugin"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:'<sensor name="imu" type="imu">\n  <pose>0 0 0 0 0 0</pose>\n  <update_rate>100</update_rate>\n  <imu>\n    <angular_velocity>\n      <x>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>0.0002</stddev>\n        </noise>\n      </x>\n      \x3c!-- Similar for y, z --\x3e\n    </angular_velocity>\n    <linear_acceleration>\n      <x>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>0.017</stddev>\n        </noise>\n      </x>\n      \x3c!-- Similar for y, z --\x3e\n    </linear_acceleration>\n  </imu>\n  <plugin name="gazebo_ros_imu" filename="libgazebo_ros_imu.so">\n    <ros>\n      <namespace>/imu</namespace>\n    </ros>\n    <topicName>/imu/data</topicName>\n    <frameName>imu_frame</frameName>\n  </plugin>\n</sensor>\n'})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"4-sensor-noise",children:"4. Sensor Noise"}),"\n",(0,a.jsx)(n.p,{children:"Real sensors have noise. Gazebo allows you to model this:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:'<noise type="gaussian">\n  <mean>0.0</mean>\n  <stddev>0.01</stddev>\n  <bias_mean>0.0</bias_mean>\n  <bias_stddev>0.0</bias_stddev>\n</noise>\n'})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"5-exercises",children:"5. Exercises"}),"\n",(0,a.jsx)(n.h3,{id:"exercise-341-process-lidar-data",children:"Exercise 3.4.1: Process LiDAR Data"}),"\n",(0,a.jsx)(n.p,{children:"Simulate processing LiDAR scan data to detect obstacles."}),"\n",(0,a.jsx)(s,{id:"ex-3-4-1",title:"LiDAR Obstacle Detection",starterCode:'import numpy as np\n\ndef detect_obstacles(ranges, min_range=0.1, max_range=30.0, threshold=1.0):\n  """\n  Detect obstacles from LiDAR ranges.\n  \n  Args:\n      ranges: Array of distance measurements\n      min_range: Minimum valid range\n      max_range: Maximum valid range\n      threshold: Distance threshold for obstacle\n  \n  Returns:\n      List of obstacle angles (indices)\n  """\n  # TODO: Filter valid ranges, find obstacles closer than threshold\n  pass\n\n# Test with sample data\nranges = np.array([2.5, 1.8, 0.5, 0.3, 0.8, 2.1, 3.0])\nobstacles = detect_obstacles(ranges, threshold=1.0)\nprint(f"Obstacles detected at angles: {obstacles}")\n',hints:["Filter ranges: valid = (ranges >= min_range) & (ranges <= max_range)","Find obstacles: obstacles = np.where(valid_ranges < threshold)[0]","Return list of indices where obstacles are detected"]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Key Takeaways"}),":"]}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Gazebo simulates realistic sensor data"}),"\n",(0,a.jsx)(n.li,{children:"LiDAR provides 3D point clouds for mapping"}),"\n",(0,a.jsx)(n.li,{children:"Depth cameras give RGB-D data for perception"}),"\n",(0,a.jsx)(n.li,{children:"IMUs measure orientation and acceleration"}),"\n",(0,a.jsx)(n.li,{children:"Sensor noise should be modeled for realism"}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"What's Next"}),": ",(0,a.jsx)(n.a,{href:"/physical-ai-book/docs/chapter-03/lesson-05-unity-rendering",children:"Lesson 3.5: Unity for High-Fidelity Rendering"})," explores photorealistic visualization."]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Estimated completion time"}),": 70 minutes | ",(0,a.jsx)(n.strong,{children:"Prerequisites"}),": Lesson 3.3 | ",(0,a.jsx)(n.strong,{children:"Difficulty"}),": B2 (Advanced)"]})]})}function m(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>t,x:()=>o});var i=s(6540);const a={},r=i.createContext(a);function t(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:t(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);