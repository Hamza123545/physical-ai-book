if(void 0===__webpack_require__)var __webpack_require__={};(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[9123],{5335:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>c,default:()=>h,frontMatter:()=>o,metadata:()=>r,toc:()=>l});const r=JSON.parse('{"id":"chapter-07/lesson-01-whisper-voice","title":"Lesson 7.1: Voice-to-Action with OpenAI Whisper","description":"Learn to use OpenAI Whisper for speech recognition and voice command processing","source":"@site/docs/chapter-07/lesson-01-whisper-voice.md","sourceDirName":"chapter-07","slug":"/chapter-07/lesson-01-whisper-voice","permalink":"/physical-ai-book/docs/chapter-07/lesson-01-whisper-voice","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"whisper","permalink":"/physical-ai-book/docs/tags/whisper"},{"inline":true,"label":"speech-recognition","permalink":"/physical-ai-book/docs/tags/speech-recognition"},{"inline":true,"label":"voice-commands","permalink":"/physical-ai-book/docs/tags/voice-commands"},{"inline":true,"label":"vla","permalink":"/physical-ai-book/docs/tags/vla"}],"version":"current","frontMatter":{"title":"Lesson 7.1: Voice-to-Action with OpenAI Whisper","description":"Learn to use OpenAI Whisper for speech recognition and voice command processing","chapter":7,"lesson":1,"estimated_time":70,"cefr_level":"B2","blooms_level":"Apply","digcomp_level":5,"generated_by":"claude-sonnet-4-5-20250929","source_spec":"specs/002-physical-ai-textbook/spec.md","created":"2025-11-29","last_modified":"2025-11-29","git_author":"hswat","workflow":"/sp.implement","version":"1.0","prerequisites":["chapter-02-lesson-02"],"has_interactive_python":true,"interactive_python_count":1,"has_try_with_ai":false,"try_with_ai_count":0,"tags":["whisper","speech-recognition","voice-commands","vla"]},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 7 Quiz","permalink":"/physical-ai-book/docs/chapter-07/quiz"},"next":{"title":"Lesson 7.2: LLM-Based Cognitive Planning","permalink":"/physical-ai-book/docs/chapter-07/lesson-02-llm-planning"}}');var s=i(4848),t=i(8453);const o={title:"Lesson 7.1: Voice-to-Action with OpenAI Whisper",description:"Learn to use OpenAI Whisper for speech recognition and voice command processing",chapter:7,lesson:1,estimated_time:70,cefr_level:"B2",blooms_level:"Apply",digcomp_level:5,generated_by:"claude-sonnet-4-5-20250929",source_spec:"specs/002-physical-ai-textbook/spec.md",created:"2025-11-29",last_modified:"2025-11-29",git_author:"hswat",workflow:"/sp.implement",version:"1.0",prerequisites:["chapter-02-lesson-02"],has_interactive_python:!0,interactive_python_count:1,has_try_with_ai:!1,try_with_ai_count:0,tags:["whisper","speech-recognition","voice-commands","vla"]},c="Lesson 7.1: Voice-to-Action with OpenAI Whisper",a={},l=[{value:"\ud83c\udfaf Learning Objectives",id:"-learning-objectives",level:2},{value:"Introduction",id:"introduction",level:2},{value:"1. OpenAI Whisper Overview",id:"1-openai-whisper-overview",level:2},{value:"Architecture",id:"architecture",level:3},{value:"Use Cases in Robotics",id:"use-cases-in-robotics",level:3},{value:"2. Whisper API Integration",id:"2-whisper-api-integration",level:2},{value:"Basic Usage",id:"basic-usage",level:3},{value:"Real-Time Transcription",id:"real-time-transcription",level:3},{value:"3. ROS 2 Integration",id:"3-ros-2-integration",level:2},{value:"Voice Command Publisher",id:"voice-command-publisher",level:3},{value:"4. Command Processing",id:"4-command-processing",level:2},{value:"Extracting Robot Commands",id:"extracting-robot-commands",level:3},{value:"5. Exercises",id:"5-exercises",level:2},{value:"Exercise 7.1.1: Basic Whisper Transcription",id:"exercise-711-basic-whisper-transcription",level:3},{value:"Summary",id:"summary",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components},{InteractivePython:i}=n;return i||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("InteractivePython",!0),(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"lesson-71-voice-to-action-with-openai-whisper",children:"Lesson 7.1: Voice-to-Action with OpenAI Whisper"})}),"\n",(0,s.jsx)(n.h2,{id:"-learning-objectives",children:"\ud83c\udfaf Learning Objectives"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Understand OpenAI Whisper architecture and capabilities"}),"\n",(0,s.jsx)(n.li,{children:"Integrate Whisper API for speech-to-text conversion"}),"\n",(0,s.jsx)(n.li,{children:"Process voice commands for robot control"}),"\n",(0,s.jsx)(n.li,{children:"Implement real-time voice command recognition"}),"\n",(0,s.jsx)(n.li,{children:"Publish voice commands to ROS 2 topics"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Time"}),": 70 minutes"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"OpenAI Whisper"})," is a state-of-the-art speech recognition system that can transcribe audio in multiple languages with high accuracy. For robotics, Whisper enables natural voice control\u2014users can speak commands instead of typing or using complex interfaces."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Key Features"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"High accuracy across languages"}),"\n",(0,s.jsx)(n.li,{children:"Handles accents and background noise"}),"\n",(0,s.jsx)(n.li,{children:"Real-time transcription capability"}),"\n",(0,s.jsx)(n.li,{children:"Free API tier available"}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"1-openai-whisper-overview",children:"1. OpenAI Whisper Overview"}),"\n",(0,s.jsx)(n.h3,{id:"architecture",children:"Architecture"}),"\n",(0,s.jsx)(n.p,{children:"Whisper uses a transformer-based architecture:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Encoder"}),": Processes audio into features"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Decoder"}),": Generates text transcription"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Multilingual"}),": Supports 99+ languages"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"use-cases-in-robotics",children:"Use Cases in Robotics"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'Voice commands: "Move forward", "Pick up the cup"'}),"\n",(0,s.jsx)(n.li,{children:'Natural language: "Can you help me clean the room?"'}),"\n",(0,s.jsx)(n.li,{children:"Multi-modal interaction: Combine voice with gestures"}),"\n",(0,s.jsx)(n.li,{children:"Accessibility: Enable voice control for users"}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"2-whisper-api-integration",children:"2. Whisper API Integration"}),"\n",(0,s.jsx)(n.h3,{id:"basic-usage",children:"Basic Usage"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import openai\n\n# Transcribe audio file\naudio_file = open("voice_command.wav", "rb")\ntranscript = openai.Audio.transcribe(\n    model="whisper-1",\n    file=audio_file,\n    language="en"\n)\nprint(transcript["text"])\n'})}),"\n",(0,s.jsx)(n.h3,{id:"real-time-transcription",children:"Real-Time Transcription"}),"\n",(0,s.jsx)(n.p,{children:"For real-time voice commands, you'll need to:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Capture audio from microphone"}),"\n",(0,s.jsx)(n.li,{children:"Send chunks to Whisper API"}),"\n",(0,s.jsx)(n.li,{children:"Process transcriptions"}),"\n",(0,s.jsx)(n.li,{children:"Extract commands"}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"3-ros-2-integration",children:"3. ROS 2 Integration"}),"\n",(0,s.jsx)(n.h3,{id:"voice-command-publisher",children:"Voice Command Publisher"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport openai\n\nclass VoiceCommandNode(Node):\n    def __init__(self):\n        super().__init__(\'voice_command_node\')\n        self.publisher = self.create_publisher(String, \'/voice_commands\', 10)\n        \n    def transcribe_audio(self, audio_file):\n        """Transcribe audio using Whisper."""\n        transcript = openai.Audio.transcribe(\n            model="whisper-1",\n            file=audio_file\n        )\n        return transcript["text"]\n    \n    def publish_command(self, text):\n        """Publish voice command to ROS 2 topic."""\n        msg = String()\n        msg.data = text\n        self.publisher.publish(msg)\n        self.get_logger().info(f\'Published command: {text}\')\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"4-command-processing",children:"4. Command Processing"}),"\n",(0,s.jsx)(n.h3,{id:"extracting-robot-commands",children:"Extracting Robot Commands"}),"\n",(0,s.jsx)(n.p,{children:"Voice commands need to be parsed and validated:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'def parse_voice_command(text):\n    """Extract robot action from natural language."""\n    text_lower = text.lower()\n    \n    # Movement commands\n    if "move forward" in text_lower or "go forward" in text_lower:\n        return {"action": "move", "direction": "forward"}\n    elif "turn left" in text_lower:\n        return {"action": "turn", "direction": "left"}\n    elif "pick up" in text_lower:\n        # Extract object name\n        return {"action": "pick", "object": extract_object(text_lower)}\n    \n    return None\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"5-exercises",children:"5. Exercises"}),"\n",(0,s.jsx)(n.h3,{id:"exercise-711-basic-whisper-transcription",children:"Exercise 7.1.1: Basic Whisper Transcription"}),"\n",(0,s.jsx)(n.p,{children:"Transcribe a sample audio file using Whisper API."}),"\n",(0,s.jsx)(i,{id:"ex-7-1-1",title:"Whisper Transcription",starterCode:'# Note: This exercise requires OpenAI API key\n# In real implementation, use: openai.Audio.transcribe()\n\ndef simulate_whisper_transcription(audio_text):\n  """\n  Simulate Whisper transcription (for browser-based exercise).\n  In real implementation, this would call OpenAI API.\n  """\n  # TODO: Simulate transcription process\n  # Return transcribed text\n  pass\n\n# Test\naudio = "Hello robot, please move forward"\ntranscript = simulate_whisper_transcription(audio)\nprint(f"Transcribed: {transcript}")\n',hints:["For simulation, return the input text (real API would process audio)","In real code, use openai.Audio.transcribe()","Handle errors and edge cases"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Key Takeaways"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Whisper provides high-accuracy speech recognition"}),"\n",(0,s.jsx)(n.li,{children:"Voice commands enable natural robot interaction"}),"\n",(0,s.jsx)(n.li,{children:"ROS 2 integration allows voice control of robots"}),"\n",(0,s.jsx)(n.li,{children:"Command parsing extracts actionable instructions"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"What's Next"}),": ",(0,s.jsx)(n.a,{href:"/physical-ai-book/docs/chapter-07/lesson-02-llm-planning",children:"Lesson 7.2: LLM-Based Cognitive Planning"})," teaches you to use GPT models for task planning."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Estimated completion time"}),": 70 minutes | ",(0,s.jsx)(n.strong,{children:"Prerequisites"}),": Chapter 2 (ROS 2) | ",(0,s.jsx)(n.strong,{children:"Difficulty"}),": B2 (Advanced)"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>c});var r=i(6540);const s={},t=r.createContext(s);function o(e){const n=r.useContext(t);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),r.createElement(t.Provider,{value:n},e.children)}}}]);