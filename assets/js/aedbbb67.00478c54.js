if(void 0===__webpack_require__)var __webpack_require__={};(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[6806],{1526:(s,e,a)=>{a.r(e),a.d(e,{assets:()=>m,contentTitle:()=>t,default:()=>o,frontMatter:()=>r,metadata:()=>n,toc:()=>c});const n=JSON.parse('{"id":"chapter-04/lesson-02-q-learning","title":"Lesson 4.2: Q-Learning Algorithm","description":"Implement tabular Q-learning for discrete state-action spaces","source":"@site/docs/chapter-04/lesson-02-q-learning.md","sourceDirName":"chapter-04","slug":"/chapter-04/lesson-02-q-learning","permalink":"/physical-ai-book/docs/chapter-04/lesson-02-q-learning","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"q-learning","permalink":"/physical-ai-book/docs/tags/q-learning"},{"inline":true,"label":"temporal-difference","permalink":"/physical-ai-book/docs/tags/temporal-difference"},{"inline":true,"label":"epsilon-greedy","permalink":"/physical-ai-book/docs/tags/epsilon-greedy"},{"inline":true,"label":"tabular-rl","permalink":"/physical-ai-book/docs/tags/tabular-rl"}],"version":"current","frontMatter":{"title":"Lesson 4.2: Q-Learning Algorithm","description":"Implement tabular Q-learning for discrete state-action spaces","chapter":4,"lesson":2,"estimated_time":60,"cefr_level":"B2","blooms_level":"Apply","digcomp_level":5,"generated_by":"claude-sonnet-4-5-20250929","source_spec":"specs/002-physical-ai-textbook/spec.md","created":"2025-11-29","last_modified":"2025-11-29","git_author":"hswat","workflow":"/sp.implement","version":"1.0","prerequisites":["chapter-04-lesson-01"],"has_interactive_python":true,"interactive_python_count":3,"has_try_with_ai":true,"try_with_ai_count":2,"tags":["q-learning","temporal-difference","epsilon-greedy","tabular-rl"]},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 4.1: RL Basics and Markov Decision Processes","permalink":"/physical-ai-book/docs/chapter-04/lesson-01-rl-basics"},"next":{"title":"Lesson 4.3: Deep Q-Networks (DQN)","permalink":"/physical-ai-book/docs/chapter-04/lesson-03-deep-q-networks"}}');var l=a(4848),i=a(8453);const r={title:"Lesson 4.2: Q-Learning Algorithm",description:"Implement tabular Q-learning for discrete state-action spaces",chapter:4,lesson:2,estimated_time:60,cefr_level:"B2",blooms_level:"Apply",digcomp_level:5,generated_by:"claude-sonnet-4-5-20250929",source_spec:"specs/002-physical-ai-textbook/spec.md",created:"2025-11-29",last_modified:"2025-11-29",git_author:"hswat",workflow:"/sp.implement",version:"1.0",prerequisites:["chapter-04-lesson-01"],has_interactive_python:!0,interactive_python_count:3,has_try_with_ai:!0,try_with_ai_count:2,tags:["q-learning","temporal-difference","epsilon-greedy","tabular-rl"]},t="Lesson 4.2: Q-Learning Algorithm",m={},c=[{value:"\ud83c\udfaf Learning Objectives",id:"-learning-objectives",level:2},{value:"Introduction",id:"introduction",level:2},{value:"1. Q-Table Representation",id:"1-q-table-representation",level:2},{value:"2. Epsilon-Greedy Policy",id:"2-epsilon-greedy-policy",level:2},{value:"Exercises",id:"exercises",level:2},{value:"Exercise 4.2.1: Q-Table Update",id:"exercise-421-q-table-update",level:3},{value:"Exercise 4.2.2: Epsilon-Greedy Policy",id:"exercise-422-epsilon-greedy-policy",level:3},{value:"Exercise 4.2.3: Train Q-Learning Agent",id:"exercise-423-train-q-learning-agent",level:3},{value:"Try With AI",id:"try-with-ai",level:2},{value:"TryWithAI 4.2.1: Hyperparameter Tuning",id:"trywithai-421-hyperparameter-tuning",level:3},{value:"TryWithAI 4.2.2: Q-Learning Debugging",id:"trywithai-422-q-learning-debugging",level:3},{value:"Summary",id:"summary",level:2}];function h(s){const e={a:"a",annotation:"annotation",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",math:"math",mi:"mi",mn:"mn",mo:"mo",mrow:"mrow",msub:"msub",msup:"msup",ol:"ol",p:"p",semantics:"semantics",span:"span",strong:"strong",ul:"ul",...(0,i.R)(),...s.components},{InteractivePython:a,LearningObjectives:n,TryWithAI:r}=e;return a||p("InteractivePython",!0),n||p("LearningObjectives",!0),r||p("TryWithAI",!0),(0,l.jsxs)(l.Fragment,{children:[(0,l.jsx)(e.header,{children:(0,l.jsx)(e.h1,{id:"lesson-42-q-learning-algorithm",children:"Lesson 4.2: Q-Learning Algorithm"})}),"\n",(0,l.jsx)(e.h2,{id:"-learning-objectives",children:"\ud83c\udfaf Learning Objectives"}),"\n",(0,l.jsx)(n,{cefr_level:"B2",objectives:[{text:"Implement Q-learning update rule",blooms_level:"Apply",assessment_method:"Q-table exercise"},{text:"Apply epsilon-greedy exploration strategy",blooms_level:"Apply",assessment_method:"Policy exercise"},{text:"Train agent to solve GridWorld with 90%+ success",blooms_level:"Apply",assessment_method:"Training exercise"}]}),"\n",(0,l.jsx)(e.h2,{id:"introduction",children:"Introduction"}),"\n",(0,l.jsxs)(e.p,{children:[(0,l.jsx)(e.strong,{children:"Q-learning"}),": Learn action-value function ",(0,l.jsxs)(e.span,{className:"katex",children:[(0,l.jsx)(e.span,{className:"katex-mathml",children:(0,l.jsx)(e.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,l.jsxs)(e.semantics,{children:[(0,l.jsxs)(e.mrow,{children:[(0,l.jsx)(e.mi,{children:"Q"}),(0,l.jsx)(e.mo,{stretchy:"false",children:"("}),(0,l.jsx)(e.mi,{children:"s"}),(0,l.jsx)(e.mo,{separator:"true",children:","}),(0,l.jsx)(e.mi,{children:"a"}),(0,l.jsx)(e.mo,{stretchy:"false",children:")"})]}),(0,l.jsx)(e.annotation,{encoding:"application/x-tex",children:"Q(s,a)"})]})})}),(0,l.jsx)(e.span,{className:"katex-html","aria-hidden":"true",children:(0,l.jsxs)(e.span,{className:"base",children:[(0,l.jsx)(e.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,l.jsx)(e.span,{className:"mord mathnormal",children:"Q"}),(0,l.jsx)(e.span,{className:"mopen",children:"("}),(0,l.jsx)(e.span,{className:"mord mathnormal",children:"s"}),(0,l.jsx)(e.span,{className:"mpunct",children:","}),(0,l.jsx)(e.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,l.jsx)(e.span,{className:"mord mathnormal",children:"a"}),(0,l.jsx)(e.span,{className:"mclose",children:")"})]})})]})," without knowing environment dynamics."]}),"\n",(0,l.jsxs)(e.p,{children:[(0,l.jsx)(e.strong,{children:"Q-update rule"}),":\n",(0,l.jsxs)(e.span,{className:"katex",children:[(0,l.jsx)(e.span,{className:"katex-mathml",children:(0,l.jsx)(e.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,l.jsxs)(e.semantics,{children:[(0,l.jsxs)(e.mrow,{children:[(0,l.jsx)(e.mi,{children:"Q"}),(0,l.jsx)(e.mo,{stretchy:"false",children:"("}),(0,l.jsx)(e.mi,{children:"s"}),(0,l.jsx)(e.mo,{separator:"true",children:","}),(0,l.jsx)(e.mi,{children:"a"}),(0,l.jsx)(e.mo,{stretchy:"false",children:")"}),(0,l.jsx)(e.mo,{children:"\u2190"}),(0,l.jsx)(e.mi,{children:"Q"}),(0,l.jsx)(e.mo,{stretchy:"false",children:"("}),(0,l.jsx)(e.mi,{children:"s"}),(0,l.jsx)(e.mo,{separator:"true",children:","}),(0,l.jsx)(e.mi,{children:"a"}),(0,l.jsx)(e.mo,{stretchy:"false",children:")"}),(0,l.jsx)(e.mo,{children:"+"}),(0,l.jsx)(e.mi,{children:"\u03b1"}),(0,l.jsx)(e.mo,{stretchy:"false",children:"["}),(0,l.jsx)(e.mi,{children:"r"}),(0,l.jsx)(e.mo,{children:"+"}),(0,l.jsx)(e.mi,{children:"\u03b3"}),(0,l.jsxs)(e.msub,{children:[(0,l.jsxs)(e.mrow,{children:[(0,l.jsx)(e.mi,{children:"max"}),(0,l.jsx)(e.mo,{children:"\u2061"})]}),(0,l.jsxs)(e.msup,{children:[(0,l.jsx)(e.mi,{children:"a"}),(0,l.jsx)(e.mo,{mathvariant:"normal",lspace:"0em",rspace:"0em",children:"\u2032"})]})]}),(0,l.jsx)(e.mi,{children:"Q"}),(0,l.jsx)(e.mo,{stretchy:"false",children:"("}),(0,l.jsxs)(e.msup,{children:[(0,l.jsx)(e.mi,{children:"s"}),(0,l.jsx)(e.mo,{mathvariant:"normal",lspace:"0em",rspace:"0em",children:"\u2032"})]}),(0,l.jsx)(e.mo,{separator:"true",children:","}),(0,l.jsxs)(e.msup,{children:[(0,l.jsx)(e.mi,{children:"a"}),(0,l.jsx)(e.mo,{mathvariant:"normal",lspace:"0em",rspace:"0em",children:"\u2032"})]}),(0,l.jsx)(e.mo,{stretchy:"false",children:")"}),(0,l.jsx)(e.mo,{children:"\u2212"}),(0,l.jsx)(e.mi,{children:"Q"}),(0,l.jsx)(e.mo,{stretchy:"false",children:"("}),(0,l.jsx)(e.mi,{children:"s"}),(0,l.jsx)(e.mo,{separator:"true",children:","}),(0,l.jsx)(e.mi,{children:"a"}),(0,l.jsx)(e.mo,{stretchy:"false",children:")"}),(0,l.jsx)(e.mo,{stretchy:"false",children:"]"})]}),(0,l.jsx)(e.annotation,{encoding:"application/x-tex",children:"Q(s,a) \\leftarrow Q(s,a) + \\alpha [r + \\gamma \\max_{a'} Q(s',a') - Q(s,a)]"})]})})}),(0,l.jsxs)(e.span,{className:"katex-html","aria-hidden":"true",children:[(0,l.jsxs)(e.span,{className:"base",children:[(0,l.jsx)(e.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,l.jsx)(e.span,{className:"mord mathnormal",children:"Q"}),(0,l.jsx)(e.span,{className:"mopen",children:"("}),(0,l.jsx)(e.span,{className:"mord mathnormal",children:"s"}),(0,l.jsx)(e.span,{className:"mpunct",children:","}),(0,l.jsx)(e.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,l.jsx)(e.span,{className:"mord mathnormal",children:"a"}),(0,l.jsx)(e.span,{className:"mclose",children:")"}),(0,l.jsx)(e.span,{className:"mspace",style:{marginRight:"0.2778em"}}),(0,l.jsx)(e.span,{className:"mrel",children:"\u2190"}),(0,l.jsx)(e.span,{className:"mspace",style:{marginRight:"0.2778em"}})]}),(0,l.jsxs)(e.span,{className:"base",children:[(0,l.jsx)(e.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,l.jsx)(e.span,{className:"mord mathnormal",children:"Q"}),(0,l.jsx)(e.span,{className:"mopen",children:"("}),(0,l.jsx)(e.span,{className:"mord mathnormal",children:"s"}),(0,l.jsx)(e.span,{className:"mpunct",children:","}),(0,l.jsx)(e.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,l.jsx)(e.span,{className:"mord mathnormal",children:"a"}),(0,l.jsx)(e.span,{className:"mclose",children:")"}),(0,l.jsx)(e.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,l.jsx)(e.span,{className:"mbin",children:"+"}),(0,l.jsx)(e.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,l.jsxs)(e.span,{className:"base",children:[(0,l.jsx)(e.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,l.jsx)(e.span,{className:"mord mathnormal",style:{marginRight:"0.0037em"},children:"\u03b1"}),(0,l.jsx)(e.span,{className:"mopen",children:"["}),(0,l.jsx)(e.span,{className:"mord mathnormal",style:{marginRight:"0.02778em"},children:"r"}),(0,l.jsx)(e.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,l.jsx)(e.span,{className:"mbin",children:"+"}),(0,l.jsx)(e.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,l.jsxs)(e.span,{className:"base",children:[(0,l.jsx)(e.span,{className:"strut",style:{height:"1.0019em",verticalAlign:"-0.25em"}}),(0,l.jsx)(e.span,{className:"mord mathnormal",style:{marginRight:"0.05556em"},children:"\u03b3"}),(0,l.jsx)(e.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,l.jsxs)(e.span,{className:"mop",children:[(0,l.jsx)(e.span,{className:"mop",children:"max"}),(0,l.jsx)(e.span,{className:"msupsub",children:(0,l.jsxs)(e.span,{className:"vlist-t vlist-t2",children:[(0,l.jsxs)(e.span,{className:"vlist-r",children:[(0,l.jsx)(e.span,{className:"vlist",style:{height:"0.328em"},children:(0,l.jsxs)(e.span,{style:{top:"-2.55em",marginRight:"0.05em"},children:[(0,l.jsx)(e.span,{className:"pstrut",style:{height:"2.7em"}}),(0,l.jsx)(e.span,{className:"sizing reset-size6 size3 mtight",children:(0,l.jsx)(e.span,{className:"mord mtight",children:(0,l.jsxs)(e.span,{className:"mord mtight",children:[(0,l.jsx)(e.span,{className:"mord mathnormal mtight",children:"a"}),(0,l.jsx)(e.span,{className:"msupsub",children:(0,l.jsx)(e.span,{className:"vlist-t",children:(0,l.jsx)(e.span,{className:"vlist-r",children:(0,l.jsx)(e.span,{className:"vlist",style:{height:"0.6828em"},children:(0,l.jsxs)(e.span,{style:{top:"-2.786em",marginRight:"0.0714em"},children:[(0,l.jsx)(e.span,{className:"pstrut",style:{height:"2.5em"}}),(0,l.jsx)(e.span,{className:"sizing reset-size3 size1 mtight",children:(0,l.jsx)(e.span,{className:"mord mtight",children:(0,l.jsx)(e.span,{className:"mord mtight",children:"\u2032"})})})]})})})})})]})})})]})}),(0,l.jsx)(e.span,{className:"vlist-s",children:"\u200b"})]}),(0,l.jsx)(e.span,{className:"vlist-r",children:(0,l.jsx)(e.span,{className:"vlist",style:{height:"0.15em"},children:(0,l.jsx)(e.span,{})})})]})})]}),(0,l.jsx)(e.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,l.jsx)(e.span,{className:"mord mathnormal",children:"Q"}),(0,l.jsx)(e.span,{className:"mopen",children:"("}),(0,l.jsxs)(e.span,{className:"mord",children:[(0,l.jsx)(e.span,{className:"mord mathnormal",children:"s"}),(0,l.jsx)(e.span,{className:"msupsub",children:(0,l.jsx)(e.span,{className:"vlist-t",children:(0,l.jsx)(e.span,{className:"vlist-r",children:(0,l.jsx)(e.span,{className:"vlist",style:{height:"0.7519em"},children:(0,l.jsxs)(e.span,{style:{top:"-3.063em",marginRight:"0.05em"},children:[(0,l.jsx)(e.span,{className:"pstrut",style:{height:"2.7em"}}),(0,l.jsx)(e.span,{className:"sizing reset-size6 size3 mtight",children:(0,l.jsx)(e.span,{className:"mord mtight",children:(0,l.jsx)(e.span,{className:"mord mtight",children:"\u2032"})})})]})})})})})]}),(0,l.jsx)(e.span,{className:"mpunct",children:","}),(0,l.jsx)(e.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,l.jsxs)(e.span,{className:"mord",children:[(0,l.jsx)(e.span,{className:"mord mathnormal",children:"a"}),(0,l.jsx)(e.span,{className:"msupsub",children:(0,l.jsx)(e.span,{className:"vlist-t",children:(0,l.jsx)(e.span,{className:"vlist-r",children:(0,l.jsx)(e.span,{className:"vlist",style:{height:"0.7519em"},children:(0,l.jsxs)(e.span,{style:{top:"-3.063em",marginRight:"0.05em"},children:[(0,l.jsx)(e.span,{className:"pstrut",style:{height:"2.7em"}}),(0,l.jsx)(e.span,{className:"sizing reset-size6 size3 mtight",children:(0,l.jsx)(e.span,{className:"mord mtight",children:(0,l.jsx)(e.span,{className:"mord mtight",children:"\u2032"})})})]})})})})})]}),(0,l.jsx)(e.span,{className:"mclose",children:")"}),(0,l.jsx)(e.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,l.jsx)(e.span,{className:"mbin",children:"\u2212"}),(0,l.jsx)(e.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,l.jsxs)(e.span,{className:"base",children:[(0,l.jsx)(e.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,l.jsx)(e.span,{className:"mord mathnormal",children:"Q"}),(0,l.jsx)(e.span,{className:"mopen",children:"("}),(0,l.jsx)(e.span,{className:"mord mathnormal",children:"s"}),(0,l.jsx)(e.span,{className:"mpunct",children:","}),(0,l.jsx)(e.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,l.jsx)(e.span,{className:"mord mathnormal",children:"a"}),(0,l.jsx)(e.span,{className:"mclose",children:")]"})]})]})]})]}),"\n",(0,l.jsxs)(e.p,{children:[(0,l.jsx)(e.strong,{children:"Key idea"}),": Bootstrap from next state's best Q-value."]}),"\n",(0,l.jsx)(e.hr,{}),"\n",(0,l.jsx)(e.h2,{id:"1-q-table-representation",children:"1. Q-Table Representation"}),"\n",(0,l.jsx)(e.p,{children:"Store Q-values in table: rows=states, columns=actions."}),"\n",(0,l.jsx)(e.hr,{}),"\n",(0,l.jsx)(e.h2,{id:"2-epsilon-greedy-policy",children:"2. Epsilon-Greedy Policy"}),"\n",(0,l.jsxs)(e.p,{children:[(0,l.jsx)(e.strong,{children:"Exploration vs. Exploitation"}),":"]}),"\n",(0,l.jsxs)(e.ul,{children:["\n",(0,l.jsxs)(e.li,{children:["With probability ",(0,l.jsxs)(e.span,{className:"katex",children:[(0,l.jsx)(e.span,{className:"katex-mathml",children:(0,l.jsx)(e.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,l.jsxs)(e.semantics,{children:[(0,l.jsx)(e.mrow,{children:(0,l.jsx)(e.mi,{children:"\u03f5"})}),(0,l.jsx)(e.annotation,{encoding:"application/x-tex",children:"\\epsilon"})]})})}),(0,l.jsx)(e.span,{className:"katex-html","aria-hidden":"true",children:(0,l.jsxs)(e.span,{className:"base",children:[(0,l.jsx)(e.span,{className:"strut",style:{height:"0.4306em"}}),(0,l.jsx)(e.span,{className:"mord mathnormal",children:"\u03f5"})]})})]}),": random action (explore)"]}),"\n",(0,l.jsxs)(e.li,{children:["With probability ",(0,l.jsxs)(e.span,{className:"katex",children:[(0,l.jsx)(e.span,{className:"katex-mathml",children:(0,l.jsx)(e.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,l.jsxs)(e.semantics,{children:[(0,l.jsxs)(e.mrow,{children:[(0,l.jsx)(e.mn,{children:"1"}),(0,l.jsx)(e.mo,{children:"\u2212"}),(0,l.jsx)(e.mi,{children:"\u03f5"})]}),(0,l.jsx)(e.annotation,{encoding:"application/x-tex",children:"1-\\epsilon"})]})})}),(0,l.jsxs)(e.span,{className:"katex-html","aria-hidden":"true",children:[(0,l.jsxs)(e.span,{className:"base",children:[(0,l.jsx)(e.span,{className:"strut",style:{height:"0.7278em",verticalAlign:"-0.0833em"}}),(0,l.jsx)(e.span,{className:"mord",children:"1"}),(0,l.jsx)(e.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,l.jsx)(e.span,{className:"mbin",children:"\u2212"}),(0,l.jsx)(e.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,l.jsxs)(e.span,{className:"base",children:[(0,l.jsx)(e.span,{className:"strut",style:{height:"0.4306em"}}),(0,l.jsx)(e.span,{className:"mord mathnormal",children:"\u03f5"})]})]})]}),": best action (exploit)"]}),"\n"]}),"\n",(0,l.jsx)(e.hr,{}),"\n",(0,l.jsx)(e.h2,{id:"exercises",children:"Exercises"}),"\n",(0,l.jsx)(e.h3,{id:"exercise-421-q-table-update",children:"Exercise 4.2.1: Q-Table Update"}),"\n",(0,l.jsx)(a,{id:"ex-4-2-1",title:"Q-Learning Update",starterCode:'import numpy as np\n\ndef q_update(Q, s, a, r, s_next, alpha=0.1, gamma=0.9):\n  """Apply Q-learning update rule."""\n  # TODO: Q[s,a] += alpha * (r + gamma * max(Q[s_next]) - Q[s,a])\n  pass\n\n# Test\nQ = np.zeros((5, 4))  # 5 states, 4 actions\nQ[0, 1] = 0.5\nq_update(Q, s=0, a=1, r=1.0, s_next=1, alpha=0.1, gamma=0.9)\nprint(f"Q[0,1]: {Q[0,1]:.3f}")\n',hints:["max_next = np.max(Q[s_next])","td_target = r + gamma * max_next","td_error = td_target - Q[s, a]","Q[s, a] += alpha * td_error"]}),"\n",(0,l.jsx)(e.hr,{}),"\n",(0,l.jsx)(e.h3,{id:"exercise-422-epsilon-greedy-policy",children:"Exercise 4.2.2: Epsilon-Greedy Policy"}),"\n",(0,l.jsx)(a,{id:"ex-4-2-2",title:"Exploration Strategy",starterCode:'import numpy as np\n\ndef epsilon_greedy(Q, state, epsilon=0.1):\n  """Select action using epsilon-greedy."""\n  # TODO: Random action with prob epsilon, else best action\n  pass\n\n# Test\nQ = np.array([[0.1, 0.5, 0.2, 0.3]])\naction = epsilon_greedy(Q, state=0, epsilon=0.1)\nprint(f"Action: {action}")\n',hints:["if np.random.rand() < epsilon:","  return np.random.randint(Q.shape[1])","else:","  return np.argmax(Q[state])"]}),"\n",(0,l.jsx)(e.hr,{}),"\n",(0,l.jsx)(e.h3,{id:"exercise-423-train-q-learning-agent",children:"Exercise 4.2.3: Train Q-Learning Agent"}),"\n",(0,l.jsx)(a,{id:"ex-4-2-3",title:"GridWorld Q-Learning",starterCode:'import numpy as np\n\ndef train_q_learning(env, episodes=500, alpha=0.1, gamma=0.9, epsilon=0.1):\n  """Train Q-learning agent."""\n  Q = np.zeros((env.n_states, env.n_actions))\n\n  # TODO: For each episode:\n  #   Reset env\n  #   While not done:\n  #     Select action (epsilon-greedy)\n  #     Take action, observe r, s\'\n  #     Update Q-table\n\n  return Q\n\n# Test (requires GridWorld from 4.1)\n# Q = train_q_learning(env, episodes=500)\n# print(f"Trained Q-table shape: {Q.shape}")\n',hints:["for episode in range(episodes):","  state = env.reset()","  done = False","  while not done:","    action = epsilon_greedy(Q, state, epsilon)","    next_state, reward, done = env.step(action)","    q_update(Q, state, action, reward, next_state, alpha, gamma)","    state = next_state"]}),"\n",(0,l.jsx)(e.hr,{}),"\n",(0,l.jsx)(e.h2,{id:"try-with-ai",children:"Try With AI"}),"\n",(0,l.jsx)(e.h3,{id:"trywithai-421-hyperparameter-tuning",children:"TryWithAI 4.2.1: Hyperparameter Tuning"}),"\n",(0,l.jsx)(r,{id:"tryai-4-2-1",title:"Tune Learning Rate and Epsilon",role:"Copilot",scenario:"Your Q-learning agent learns slowly or fails to converge.",yourTask:"Train agent with different alpha (0.01, 0.1, 0.5) and epsilon (0.05, 0.1, 0.3). Plot learning curves.",aiPromptTemplate:"My Q-learning agent learns slowly. With alpha=0.1, epsilon=0.1, it needs 500 episodes. Here's my code: [paste]. Can you help tune hyperparameters? What's the role of alpha vs epsilon? How do I detect convergence?",successCriteria:["You understand alpha controls learning speed","You know epsilon balances exploration/exploitation","You can tune hyperparameters systematically"],reflectionQuestions:["What happens if alpha is too high?","Should epsilon decay over time?","How many episodes are enough?"]}),"\n",(0,l.jsx)(e.hr,{}),"\n",(0,l.jsx)(e.h3,{id:"trywithai-422-q-learning-debugging",children:"TryWithAI 4.2.2: Q-Learning Debugging"}),"\n",(0,l.jsx)(r,{id:"tryai-4-2-2",title:"Debug Q-Table",role:"Evaluator",scenario:"Your Q-table has unexpected values or agent takes bad actions.",yourTask:"Complete Exercise 4.2.3. Inspect Q-table after training. Verify optimal policy.",aiPromptTemplate:"My Q-learning agent trained but takes suboptimal actions. Here's my Q-table: [paste values]. Can you review: (1) Are Q-values reasonable? (2) Does argmax(Q) give optimal policy? (3) Common bugs in my update rule?",successCriteria:["You can interpret Q-values","You verify optimal policy from Q-table","You know common Q-learning bugs"],reflectionQuestions:["What Q-values indicate good/bad states?","How to visualize Q-table?","When is off-policy learning useful?"]}),"\n",(0,l.jsx)(e.hr,{}),"\n",(0,l.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,l.jsxs)(e.ol,{children:["\n",(0,l.jsxs)(e.li,{children:[(0,l.jsx)(e.strong,{children:"Q-Learning"}),": Model-free TD learning"]}),"\n",(0,l.jsxs)(e.li,{children:[(0,l.jsx)(e.strong,{children:"Update"}),": ",(0,l.jsxs)(e.span,{className:"katex",children:[(0,l.jsx)(e.span,{className:"katex-mathml",children:(0,l.jsx)(e.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,l.jsxs)(e.semantics,{children:[(0,l.jsxs)(e.mrow,{children:[(0,l.jsx)(e.mi,{children:"Q"}),(0,l.jsx)(e.mo,{stretchy:"false",children:"("}),(0,l.jsx)(e.mi,{children:"s"}),(0,l.jsx)(e.mo,{separator:"true",children:","}),(0,l.jsx)(e.mi,{children:"a"}),(0,l.jsx)(e.mo,{stretchy:"false",children:")"}),(0,l.jsx)(e.mo,{children:"\u2190"}),(0,l.jsx)(e.mi,{children:"Q"}),(0,l.jsx)(e.mo,{stretchy:"false",children:"("}),(0,l.jsx)(e.mi,{children:"s"}),(0,l.jsx)(e.mo,{separator:"true",children:","}),(0,l.jsx)(e.mi,{children:"a"}),(0,l.jsx)(e.mo,{stretchy:"false",children:")"}),(0,l.jsx)(e.mo,{children:"+"}),(0,l.jsx)(e.mi,{children:"\u03b1"}),(0,l.jsx)(e.mo,{stretchy:"false",children:"["}),(0,l.jsx)(e.mi,{children:"r"}),(0,l.jsx)(e.mo,{children:"+"}),(0,l.jsx)(e.mi,{children:"\u03b3"}),(0,l.jsxs)(e.msub,{children:[(0,l.jsxs)(e.mrow,{children:[(0,l.jsx)(e.mi,{children:"max"}),(0,l.jsx)(e.mo,{children:"\u2061"})]}),(0,l.jsxs)(e.msup,{children:[(0,l.jsx)(e.mi,{children:"a"}),(0,l.jsx)(e.mo,{mathvariant:"normal",lspace:"0em",rspace:"0em",children:"\u2032"})]})]}),(0,l.jsx)(e.mi,{children:"Q"}),(0,l.jsx)(e.mo,{stretchy:"false",children:"("}),(0,l.jsxs)(e.msup,{children:[(0,l.jsx)(e.mi,{children:"s"}),(0,l.jsx)(e.mo,{mathvariant:"normal",lspace:"0em",rspace:"0em",children:"\u2032"})]}),(0,l.jsx)(e.mo,{separator:"true",children:","}),(0,l.jsxs)(e.msup,{children:[(0,l.jsx)(e.mi,{children:"a"}),(0,l.jsx)(e.mo,{mathvariant:"normal",lspace:"0em",rspace:"0em",children:"\u2032"})]}),(0,l.jsx)(e.mo,{stretchy:"false",children:")"}),(0,l.jsx)(e.mo,{children:"\u2212"}),(0,l.jsx)(e.mi,{children:"Q"}),(0,l.jsx)(e.mo,{stretchy:"false",children:"("}),(0,l.jsx)(e.mi,{children:"s"}),(0,l.jsx)(e.mo,{separator:"true",children:","}),(0,l.jsx)(e.mi,{children:"a"}),(0,l.jsx)(e.mo,{stretchy:"false",children:")"}),(0,l.jsx)(e.mo,{stretchy:"false",children:"]"})]}),(0,l.jsx)(e.annotation,{encoding:"application/x-tex",children:"Q(s,a) \\leftarrow Q(s,a) + \\alpha [r + \\gamma \\max_{a'} Q(s',a') - Q(s,a)]"})]})})}),(0,l.jsxs)(e.span,{className:"katex-html","aria-hidden":"true",children:[(0,l.jsxs)(e.span,{className:"base",children:[(0,l.jsx)(e.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,l.jsx)(e.span,{className:"mord mathnormal",children:"Q"}),(0,l.jsx)(e.span,{className:"mopen",children:"("}),(0,l.jsx)(e.span,{className:"mord mathnormal",children:"s"}),(0,l.jsx)(e.span,{className:"mpunct",children:","}),(0,l.jsx)(e.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,l.jsx)(e.span,{className:"mord mathnormal",children:"a"}),(0,l.jsx)(e.span,{className:"mclose",children:")"}),(0,l.jsx)(e.span,{className:"mspace",style:{marginRight:"0.2778em"}}),(0,l.jsx)(e.span,{className:"mrel",children:"\u2190"}),(0,l.jsx)(e.span,{className:"mspace",style:{marginRight:"0.2778em"}})]}),(0,l.jsxs)(e.span,{className:"base",children:[(0,l.jsx)(e.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,l.jsx)(e.span,{className:"mord mathnormal",children:"Q"}),(0,l.jsx)(e.span,{className:"mopen",children:"("}),(0,l.jsx)(e.span,{className:"mord mathnormal",children:"s"}),(0,l.jsx)(e.span,{className:"mpunct",children:","}),(0,l.jsx)(e.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,l.jsx)(e.span,{className:"mord mathnormal",children:"a"}),(0,l.jsx)(e.span,{className:"mclose",children:")"}),(0,l.jsx)(e.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,l.jsx)(e.span,{className:"mbin",children:"+"}),(0,l.jsx)(e.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,l.jsxs)(e.span,{className:"base",children:[(0,l.jsx)(e.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,l.jsx)(e.span,{className:"mord mathnormal",style:{marginRight:"0.0037em"},children:"\u03b1"}),(0,l.jsx)(e.span,{className:"mopen",children:"["}),(0,l.jsx)(e.span,{className:"mord mathnormal",style:{marginRight:"0.02778em"},children:"r"}),(0,l.jsx)(e.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,l.jsx)(e.span,{className:"mbin",children:"+"}),(0,l.jsx)(e.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,l.jsxs)(e.span,{className:"base",children:[(0,l.jsx)(e.span,{className:"strut",style:{height:"1.0019em",verticalAlign:"-0.25em"}}),(0,l.jsx)(e.span,{className:"mord mathnormal",style:{marginRight:"0.05556em"},children:"\u03b3"}),(0,l.jsx)(e.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,l.jsxs)(e.span,{className:"mop",children:[(0,l.jsx)(e.span,{className:"mop",children:"max"}),(0,l.jsx)(e.span,{className:"msupsub",children:(0,l.jsxs)(e.span,{className:"vlist-t vlist-t2",children:[(0,l.jsxs)(e.span,{className:"vlist-r",children:[(0,l.jsx)(e.span,{className:"vlist",style:{height:"0.328em"},children:(0,l.jsxs)(e.span,{style:{top:"-2.55em",marginRight:"0.05em"},children:[(0,l.jsx)(e.span,{className:"pstrut",style:{height:"2.7em"}}),(0,l.jsx)(e.span,{className:"sizing reset-size6 size3 mtight",children:(0,l.jsx)(e.span,{className:"mord mtight",children:(0,l.jsxs)(e.span,{className:"mord mtight",children:[(0,l.jsx)(e.span,{className:"mord mathnormal mtight",children:"a"}),(0,l.jsx)(e.span,{className:"msupsub",children:(0,l.jsx)(e.span,{className:"vlist-t",children:(0,l.jsx)(e.span,{className:"vlist-r",children:(0,l.jsx)(e.span,{className:"vlist",style:{height:"0.6828em"},children:(0,l.jsxs)(e.span,{style:{top:"-2.786em",marginRight:"0.0714em"},children:[(0,l.jsx)(e.span,{className:"pstrut",style:{height:"2.5em"}}),(0,l.jsx)(e.span,{className:"sizing reset-size3 size1 mtight",children:(0,l.jsx)(e.span,{className:"mord mtight",children:(0,l.jsx)(e.span,{className:"mord mtight",children:"\u2032"})})})]})})})})})]})})})]})}),(0,l.jsx)(e.span,{className:"vlist-s",children:"\u200b"})]}),(0,l.jsx)(e.span,{className:"vlist-r",children:(0,l.jsx)(e.span,{className:"vlist",style:{height:"0.15em"},children:(0,l.jsx)(e.span,{})})})]})})]}),(0,l.jsx)(e.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,l.jsx)(e.span,{className:"mord mathnormal",children:"Q"}),(0,l.jsx)(e.span,{className:"mopen",children:"("}),(0,l.jsxs)(e.span,{className:"mord",children:[(0,l.jsx)(e.span,{className:"mord mathnormal",children:"s"}),(0,l.jsx)(e.span,{className:"msupsub",children:(0,l.jsx)(e.span,{className:"vlist-t",children:(0,l.jsx)(e.span,{className:"vlist-r",children:(0,l.jsx)(e.span,{className:"vlist",style:{height:"0.7519em"},children:(0,l.jsxs)(e.span,{style:{top:"-3.063em",marginRight:"0.05em"},children:[(0,l.jsx)(e.span,{className:"pstrut",style:{height:"2.7em"}}),(0,l.jsx)(e.span,{className:"sizing reset-size6 size3 mtight",children:(0,l.jsx)(e.span,{className:"mord mtight",children:(0,l.jsx)(e.span,{className:"mord mtight",children:"\u2032"})})})]})})})})})]}),(0,l.jsx)(e.span,{className:"mpunct",children:","}),(0,l.jsx)(e.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,l.jsxs)(e.span,{className:"mord",children:[(0,l.jsx)(e.span,{className:"mord mathnormal",children:"a"}),(0,l.jsx)(e.span,{className:"msupsub",children:(0,l.jsx)(e.span,{className:"vlist-t",children:(0,l.jsx)(e.span,{className:"vlist-r",children:(0,l.jsx)(e.span,{className:"vlist",style:{height:"0.7519em"},children:(0,l.jsxs)(e.span,{style:{top:"-3.063em",marginRight:"0.05em"},children:[(0,l.jsx)(e.span,{className:"pstrut",style:{height:"2.7em"}}),(0,l.jsx)(e.span,{className:"sizing reset-size6 size3 mtight",children:(0,l.jsx)(e.span,{className:"mord mtight",children:(0,l.jsx)(e.span,{className:"mord mtight",children:"\u2032"})})})]})})})})})]}),(0,l.jsx)(e.span,{className:"mclose",children:")"}),(0,l.jsx)(e.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,l.jsx)(e.span,{className:"mbin",children:"\u2212"}),(0,l.jsx)(e.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,l.jsxs)(e.span,{className:"base",children:[(0,l.jsx)(e.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,l.jsx)(e.span,{className:"mord mathnormal",children:"Q"}),(0,l.jsx)(e.span,{className:"mopen",children:"("}),(0,l.jsx)(e.span,{className:"mord mathnormal",children:"s"}),(0,l.jsx)(e.span,{className:"mpunct",children:","}),(0,l.jsx)(e.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,l.jsx)(e.span,{className:"mord mathnormal",children:"a"}),(0,l.jsx)(e.span,{className:"mclose",children:")]"})]})]})]})]}),"\n",(0,l.jsxs)(e.li,{children:[(0,l.jsx)(e.strong,{children:"Exploration"}),": Epsilon-greedy balances exploration/exploitation"]}),"\n"]}),"\n",(0,l.jsxs)(e.p,{children:[(0,l.jsx)(e.strong,{children:"Next"}),": ",(0,l.jsx)(e.a,{href:"/physical-ai-book/docs/chapter-04/lesson-03-deep-q-networks",children:"Lesson 4.3: Deep Q-Networks"})]})]})}function o(s={}){const{wrapper:e}={...(0,i.R)(),...s.components};return e?(0,l.jsx)(e,{...s,children:(0,l.jsx)(h,{...s})}):h(s)}function p(s,e){throw new Error("Expected "+(e?"component":"object")+" `"+s+"` to be defined: you likely forgot to import, pass, or provide it.")}},8453:(s,e,a)=>{a.d(e,{R:()=>r,x:()=>t});var n=a(6540);const l={},i=n.createContext(l);function r(s){const e=n.useContext(i);return n.useMemo(function(){return"function"==typeof s?s(e):{...e,...s}},[e,s])}function t(s){let e;return e=s.disableParentContext?"function"==typeof s.components?s.components(l):s.components||l:r(s.components),n.createElement(i.Provider,{value:e},s.children)}}}]);